,Unnamed: 0.1,Unnamed: 0,제목,저자,초록,게재일,한 줄 요약,키워드
0,0,0,Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts,"['Övgü Özdemir', 'Erdem Akagündüz']","Visual question answering (VQA) is known as an AI-complete task as it requires understanding, reasoning, and inferring about the vision and the language content. Over the past few years, numerous neural architectures have been suggested for the VQA problem. However, achieving success in zero-shot VQA remains a challenge due to its requirement for advanced generalization and reasoning skills. This study explores the impact of incorporating image captioning as an intermediary process within the VQA pipeline. Specifically, we explore the efficacy of utilizing image captions instead of images and leveraging large language models (LLMs) to establish a zero-shot setting. Since image captioning is the most crucial step in this process, we compare the impact of state-of-the-art image captioning models on VQA performance across various question types in terms of structure and semantics. We propose a straightforward and efficient question-driven image captioning approach within this pipeline to transfer contextual information into the question-answering (QA) model. This method involves extracting keywords from the question, generating a caption for each image-question pair using the keywords, and incorporating the question-driven caption into the LLM prompt. We evaluate the efficacy of using general-purpose and question-driven image captions in the VQA pipeline. Our study highlights the potential of employing image captions and harnessing the capabilities of LLMs to achieve competitive performance on GQA under the zero-shot setting. Our code is available at \url{this https URL}.","Fri, 12 Apr 2024 16:35:23 UTC",": Visual question answering, AI-complete task, neural architectures, zero-shot VQA, image captioning, large language models, question-driven approach, keywords extraction, GQA.

한 줄 요약: 본 연구는 이미지 캡션을 활용하여 영상 질문 응답 시스템의 성능을 향상시키는 방법을 탐구하며, 특히 키워드 추출 및 질문 주도 이미지 캡션을 활용하여 제로샷 설정에서 경쟁력 있는 성과를 달성할 수 있는 가능성을 강조하고 있습니다.","['Visual question answering', 'AI-complete task', 'neural architectures', 'zero-shot VQA', 'image captioning', 'large language models', 'question-driven approach', 'keywords extraction', 'GQA.']"
1,1,1,Small Models Are (Still) Effective Cross-Domain Argument Extractors,"['William Gantt', 'Aaron Steven White']","Effective ontology transfer has been a major goal of recent work on event argument extraction (EAE). Two methods in particular -- question answering (QA) and template infilling (TI) -- have emerged as promising approaches to this problem. However, detailed explorations of these techniques' ability to actually enable this transfer are lacking. In this work, we provide such a study, exploring zero-shot transfer using both techniques on six major EAE datasets at both the sentence and document levels. Further, we challenge the growing reliance on LLMs for zero-shot extraction, showing that vastly smaller models trained on an appropriate source ontology can yield zero-shot performance superior to that of GPT-3.5 or GPT-4.","Fri, 12 Apr 2024 16:23:41 UTC",": ontology transfer, event argument extraction, question answering, template infilling, zero-shot transfer, LLMs, GPT-3.5, GPT-4

한국어 요약: 최근 이벤트 인자 추출(EAE)에 대한 효과적인 온톨로지 전송은 주요 목표이며, 질문 응답(QA) 및 템플릿 채움(TI)과 같은 두 가지 방법이 이 문제에 대한 유망한 접근법으로 나타났지만, 이러한 기술들이 실제 전송을 가능하게 하는 능력에 대한 상세한 탐구가 부족하다.","['ontology transfer', 'event argument extraction', 'question answering', 'template infilling', 'zero-shot transfer', 'LLMs', 'GPT-3.5', 'GPT-4']"
2,2,2,Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation,"['Hanlin Tian', 'Kethan Reddy', 'Yuxiang Feng', 'Mohammed Quddus', 'Yiannis Demiris', 'Panagiotis Angeloudis']","This paper introduces CRITICAL, a novel closed-loop framework for autonomous vehicle (AV) training and testing. CRITICAL stands out for its ability to generate diverse scenarios, focusing on critical driving situations that target specific learning and performance gaps identified in the Reinforcement Learning (RL) agent. The framework achieves this by integrating real-world traffic dynamics, driving behavior analysis, surrogate safety measures, and an optional Large Language Model (LLM) component. It is proven that the establishment of a closed feedback loop between the data generation pipeline and the training process can enhance the learning rate during training, elevate overall system performance, and augment safety resilience. Our evaluations, conducted using the Proximal Policy Optimization (PPO) and the HighwayEnv simulation environment, demonstrate noticeable performance improvements with the integration of critical case generation and LLM analysis, indicating CRITICAL's potential to improve the robustness of AV systems and streamline the generation of critical scenarios. This ultimately serves to hasten the development of AV agents, expand the general scope of RL training, and ameliorate validation efforts for AV safety.","Fri, 12 Apr 2024 16:13:10 UTC","본 논문은 자율 주행 차량(AV) 훈련 및 테스트를 위한 새로운 폐쇄 루프 프레임워크인 CRITICAL을 소개하며, 이는 다양한 시나리오를 생성하고 특정 학습 및 성능 갭에 초점을 맞춘 비평적 운전 상황을 타겟팅하여 학습률을 향상시키고 시스템 성능을 높이며 안전 저항력을 증진시킬 수 있는 능력을 갖추고 있음을 입증합니다.","['CRITICAL', 'autonomous vehicle training', 'closed-loop framework', 'diverse scenarios', 'critical driving situations', 'reinforcement learning', 'real-world traffic dynamics', 'surrogate safety measures', 'Large Language Model (LLM)', 'Proximal Policy Optimization (PPO)', 'HighwayEnv simulation environment.']"
3,3,3,RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs,"['Shreyas Chaudhari', 'Pranjal Aggarwal', 'Vishvak Murahari', 'Tanmay Rajpurohit', 'Ashwin Kalyan', 'Karthik Narasimhan', 'Ameet Deshpande', 'Bruno Castro da Silva']","State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.","Fri, 12 Apr 2024 15:54:15 UTC",": large language models, reinforcement learning from human feedback, reward model, training algorithms, limitations, performance.

한줄 요약: 이 논문은 인간 피드백을 이용한 강화 학습을 통해 대규모 언어 모델을 훈련시키는 방법과 그 한계를 분석하며, 보상 모델의 중요성과 훈련 방법의 한계를 강조하고 있습니다.","['large language models', 'reinforcement learning from human feedback', 'reward model', 'training algorithms', 'limitations', 'performance.']"
4,4,4,Memory Traces: Are Transformers Tulving Machines?,['Jean-Marie Chauvet'],"Memory traces--changes in the memory system that result from the perception and encoding of an event--were measured in pioneering studies by Endel Tulving and Michael J. Watkins in 1975. These and further experiments informed the maturation of Tulving's memory model, from the GAPS (General Abstract Processing System} to the SPI (Serial-Parallel Independent) model. Having current top of the line LLMs revisit the original Tulving-Watkins tests may help in assessing whether foundation models completely instantiate or not this class of psychological models.","Fri, 12 Apr 2024 15:37:35 UTC",": Memory traces, Endel Tulving, Michael J. Watkins, memory model, GAPS, SPI, LLMs, psychological models.

한국어 요약: 1975년 Endel Tulving과 Michael J. Watkins에 의해 이뤄진 선도적 연구에서는 기억 흔적이 측정되었으며, 이후 실험을 통해 Tulving의 기억 모델이 발전하였고, 최신 LLMs가 Tulving-Watkins 테스트를 재방문함으로써 심리학 모델의 이러한 클래스가 완전히 구현되는지 평가하는 데 도움이 될 수 있습니다.","['Memory traces', 'Endel Tulving', 'Michael J. Watkins', 'memory model', 'GAPS', 'SPI', 'LLMs', 'psychological models.']"
5,5,5,"Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward","['Xuan Xie', 'Jiayang Song', 'Zhehua Zhou', 'Yuheng Huang', 'Da Song', 'Lei Ma']","While Large Language Models (LLMs) have seen widespread applications across numerous fields, their limited interpretability poses concerns regarding their safe operations from multiple aspects, e.g., truthfulness, robustness, and fairness. Recent research has started developing quality assurance methods for LLMs, introducing techniques such as offline detector-based or uncertainty estimation methods. However, these approaches predominantly concentrate on post-generation analysis, leaving the online safety analysis for LLMs during the generation phase an unexplored area. To bridge this gap, we conduct in this work a comprehensive evaluation of the effectiveness of existing online safety analysis methods on LLMs. We begin with a pilot study that validates the feasibility of detecting unsafe outputs in the early generation process. Following this, we establish the first publicly available benchmark of online safety analysis for LLMs, including a broad spectrum of methods, models, tasks, datasets, and evaluation metrics. Utilizing this benchmark, we extensively analyze the performance of state-of-the-art online safety analysis methods on both open-source and closed-source LLMs. This analysis reveals the strengths and weaknesses of individual methods and offers valuable insights into selecting the most appropriate method based on specific application scenarios and task requirements. Furthermore, we also explore the potential of using hybridization methods, i.e., combining multiple methods to derive a collective safety conclusion, to enhance the efficacy of online safety analysis for LLMs. Our findings indicate a promising direction for the development of innovative and trustworthy quality assurance methodologies for LLMs, facilitating their reliable deployments across diverse domains.","Fri, 12 Apr 2024 14:55:16 UTC",": Large Language Models (LLMs), interpretability, safe operations, quality assurance methods, online safety analysis, benchmark, evaluation, state-of-the-art methods, hybridization methods, reliability.

Summary in Korean: 대형 언어 모델(Large Language Models, LLMs)은 널리 사용되고 있지만 해석 가능성이 제한되어 있어 안전한 운영에 대한 우려가 있습니다. 이 연구에서는 LLMs에 대한 온라인 안전 분석 방법의 효과를 평가하고, 다양한 방법과 모델을 고려하여 신뢰성 있는 배포를 위한 혁신적이고 신뢰할 수 있는 품질 보증 방법의 방향을 제시합니다.","['Large Language Models (LLMs)', 'interpretability', 'safe operations', 'quality assurance methods', 'online safety analysis', 'benchmark', 'evaluation', 'state-of-the-art methods', 'hybridization methods', 'reliability.']"
6,6,6,Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction,"['Haoran Qiu', 'Weichao Mao', 'Archit Patke', 'Shengkun Cui', 'Saurabh Jha', 'Chen Wang', 'Hubertus Franke', 'Zbigniew T. Kalbarczyk', 'Tamer Başar', 'Ravishankar K. Iyer']","Large language models (LLMs) have been driving a new wave of interactive AI applications across numerous domains. However, efficiently serving LLM inference requests is challenging due to their unpredictable execution times originating from the autoregressive nature of generative models. Existing LLM serving systems exploit first-come-first-serve (FCFS) scheduling, suffering from head-of-line blocking issues. To address the non-deterministic nature of LLMs and enable efficient interactive LLM serving, we present a speculative shortest-job-first (SSJF) scheduler that uses a light proxy model to predict LLM output sequence lengths. Our open-source SSJF implementation does not require changes to memory management or batching strategies. Evaluations on real-world datasets and production workload traces show that SSJF reduces average job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x compared to FCFS schedulers, across no batching, dynamic batching, and continuous batching settings.","Fri, 12 Apr 2024 14:46:15 UTC",": Large language models, interactive AI applications, LLM inference requests, autoregressive nature, serving systems, speculative shortest-job-first scheduler, light proxy model, memory management, batching strategies, real-world datasets, production workload traces.

Summary in Korean: 대형 언어 모델(Large language models, LLMs)은 새로운 상호작용형 AI 응용 프로그램을 주도하고 있으며, 이에 대한 효율적인 서비스를 위해 비결정적인 특성을 가진 LLM에 대한 스펙큘러티브 최단 작업 우선 스케줄러(SSJF)를 제안하고, 실제 데이터셋 및 제품 워크로드 추적을 통해 평균 작업 완료 시간을 30.5-39.6% 줄이고 처리량을 2.2-3.6배 증가시킨다.","['Large language models', 'interactive AI applications', 'LLM inference requests', 'autoregressive nature', 'serving systems', 'speculative shortest-job-first scheduler', 'light proxy model', 'memory management', 'batching strategies', 'real-world datasets', 'production workload traces.']"
7,7,7,Strategic Interactions between Large Language Models-based Agents in Beauty Contests,['Siting Lu'],"The growing adoption of large language models (LLMs) presents substantial potential for deeper understanding of human behaviours within game theory frameworks through simulations. Leveraging on the diverse pool of LLM types and addressing the gap in research on competitive games, this paper examines the strategic interactions among multiple types of LLM-based agents in a classical game of beauty contest. Drawing parallels to experiments involving human subjects, LLM-based agents are assessed similarly in terms of strategic levels. They demonstrate varying depth of reasoning that falls within a range of level-0 and 1, and show convergence in actions in repeated settings. Furthermore, I also explore how variations in group composition of agent types influence strategic behaviours, where I found higher proportion of fixed-strategy opponents enhances convergence for LLM-based agents, and having a mixed environment with agents of differing relative strategic levels accelerates convergence for all agents. There could also be higher average payoffs for the more intelligent agents, albeit at the expense of the less intelligent agents. These results not only provide insights into outcomes for simulated agents under specified scenarios, it also offer valuable implications for understanding strategic interactions between algorithms.","Fri, 12 Apr 2024 14:20:57 UTC",": large language models (LLMs), game theory, simulations, strategic interactions, beauty contest, agent types, group composition, strategic behaviors, convergence, average payoffs, algorithms

한국어 요약: 대규모 언어 모델(Large Language Models, LLMs)의 증가된 채택으로 시뮬레이션을 통해 게임 이론 프레임워크 내에서 인간 행동에 대한 깊은 이해의 잠재력이 제시되며, 이 논문은 다양한 LLM 유형을 활용하고 경쟁 게임에 대한 연구의 공백을 채우기 위해 다양한 유형의 LLM 기반 에이전트들 간의 전략적 상호작용을 분석합니다.","['large language models (LLMs)', 'game theory', 'simulations', 'strategic interactions', 'beauty contest', 'agent types', 'group composition', 'strategic behaviors', 'convergence', 'average payoffs', 'algorithms']"
8,8,8,Thematic Analysis with Large Language Models: does it work with languages other than English? A targeted test in Italian,['Stefano De Paoli'],"This paper proposes a test to perform Thematic Analysis (TA) with Large Language Model (LLM) on data which is in a different language than English. While there has been initial promising work on using pre-trained LLMs for TA on data in English, we lack any tests on whether these models can reasonably perform the same analysis with good quality in other language. In this paper a test will be proposed using an open access dataset of semi-structured interviews in Italian. The test shows that a pre-trained model can perform such a TA on the data, also using prompts in Italian. A comparative test shows the model capacity to produce themes which have a good resemblance with those produced independently by human researchers. The main implication of this study is that pre-trained LLMs may thus be suitable to support analysis in multilingual situations, so long as the language is supported by the model used.","Fri, 12 Apr 2024 14:10:09 UTC",": Thematic Analysis, Large Language Model, multilingual, Italian, pre-trained model, semi-structured interviews, comparative test

한글 요약: 본 논문은 다른 언어로 된 데이터에 대해 대규모 언어 모델(Large Language Model, LLM)을 사용하여 주제 분석(Thematic Analysis, TA)을 수행하는 테스트를 제안한다. 이 연구는 이탈리아어로 된 반구조화된 인터뷰 데이터를 활용하여 LLM이 TA를 수행할 수 있는지 테스트하고, 사람 연구자가 독립적으로 생성한 주제와 유사한 주제를 생성할 수 있다는 것을 보여준다.","['Thematic Analysis', 'Large Language Model', 'multilingual', 'Italian', 'pre-trained model', 'semi-structured interviews', 'comparative test']"
9,9,9,Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task,"['Hassan Ali', 'Philipp Allgeuer', 'Stefan Wermter']","Intention-based Human-Robot Interaction (HRI) systems allow robots to perceive and interpret user actions to proactively interact with humans and adapt to their behavior. Therefore, intention prediction is pivotal in creating a natural interactive collaboration between humans and robots. In this paper, we examine the use of Large Language Models (LLMs) for inferring human intention during a collaborative object categorization task with a physical robot. We introduce a hierarchical approach for interpreting user non-verbal cues, like hand gestures, body poses, and facial expressions and combining them with environment states and user verbal cues captured using an existing Automatic Speech Recognition (ASR) system. Our evaluation demonstrates the potential of LLMs to interpret non-verbal cues and to combine them with their context-understanding capabilities and real-world knowledge to support intention prediction during human-robot interaction.","Fri, 12 Apr 2024 12:15:14 UTC",": Intention-based Human-Robot Interaction, Large Language Models (LLMs), human intention, collaborative object categorization, non-verbal cues, Automatic Speech Recognition (ASR).

한국어 요약: 이 논문에서는 대형 언어 모델(Large Language Models, LLMs)을 사용하여 로봇과의 협업적 물체 분류 작업 중 사람의 의도를 추론하는 것을 검토하며, 비언어적 단서와 환경 상태를 해석하고 의도 예측을 지원하기 위해 LLMs의 잠재력을 시연하였다.","['Intention-based Human-Robot Interaction', 'Large Language Models (LLMs)', 'human intention', 'collaborative object categorization', 'non-verbal cues', 'Automatic Speech Recognition (ASR).']"
10,10,10,AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees,"['William Fleshman', 'Aleem Khan', 'Marc Marone', 'Benjamin Van Durme']","Large language models (LLMs) are increasingly capable of completing knowledge intensive tasks by recalling information from a static pretraining corpus. Here we are concerned with LLMs in the context of evolving data requirements. For instance: batches of new data that are introduced periodically; subsets of data with user-based access controls; or requirements on dynamic removal of documents with guarantees that associated knowledge cannot be recalled. We wish to satisfy these requirements while at the same time ensuring a model does not forget old information when new data becomes available. To address these issues, we introduce AdapterSwap, a training and inference scheme that organizes knowledge from a data collection into a set of low-rank adapters, which are dynamically composed during inference. Our experiments demonstrate AdapterSwap's ability to support efficient continual learning, while also enabling organizations to have fine-grained control over data access and deletion.","Fri, 12 Apr 2024 12:06:02 UTC",AdapterSwap는 데이터 수집 지식을 저차원 어댑터로 구성하여 동적으로 조합하고 효율적인 지속적 학습을 지원하며 조직이 데이터 접근과 삭제를 세밀하게 제어할 수 있도록 하는 것을 실험을 통해 입증하였습니다.,"['Large language models', 'evolving data requirements', 'AdapterSwap', 'continual learning', 'data access', 'deletion']"
11,11,11,Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think,"['Xinpeng Wang', 'Chengzhi Hu', 'Bolei Ma', 'Paul Röttger', 'Barbara Plank']","Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token prediction. An alternative way is to examine the text output. Prior work has shown that first token probabilities lack robustness to changes in MCQ phrasing, and that first token probabilities do not match text answers for instruction-tuned models. Therefore, in this paper, we investigate the robustness of text answers. We show that the text answers are more robust to question perturbations than the first token probabilities, when the first token answers mismatch the text answers. The difference in robustness increases as the mismatch rate becomes greater. As the mismatch reaches over 50\%, the text answer is more robust to option order changes than the debiased first token probabilities using state-of-the-art debiasing methods such as PriDe. Our findings provide further evidence for the benefits of text answer evaluation over first token probability evaluation.","Fri, 12 Apr 2024 10:36:15 UTC","이 논문에서는 텍스트 답변이 첫 번째 토큰 확률보다 질문 변형에 대해 더 강건함을 입증하며, 이러한 차이는 불일치율이 커질수록 더 커지는 것을 보여줌으로써 텍스트 답변 평가의 이점을 더욱 강조한다.","['Multiple choice questions', 'large language models', 'log probability', 'text output', 'robustness', 'question perturbations', 'mismatch rate', 'debiasing methods', 'PriDe.']"
12,12,12,Toward a Theory of Tokenization in LLMs,"['Nived Rajaraman', 'Jiantao Jiao', 'Kannan Ramchandran']","While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple k^{\text{th}}-order Markov processes for k > 1, transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from k^{\text{th}}-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.","Fri, 12 Apr 2024 09:01:14 UTC",": tokenization, language modeling, transformers, Markov processes, unigram model, cross-entropy loss.

요약: 이 논문에서는 토큰화가 상태-of-the-art 성능을 갖는 언어 모델을 디자인하기 위한 필요한 초기 단계임을 이론적인 측면에서 조사하고, 트랜스포머가 토큰화 없이 학습할 때 발생하는 문제를 분석하여 토큰화의 실제적인 필요성을 입증하였습니다.","['tokenization', 'language modeling', 'transformers', 'Markov processes', 'unigram model', 'cross-entropy loss.']"
13,13,13,Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts,"['Tianyu Zhang', 'Zixuan Zhao', 'Jiaqi Huang', 'Jingyu Hua', 'Sheng Zhong']","As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and more attention, it is of great significance to raise a generalized research paradigm to evaluate attack strengths and a basic model to conduct subtler experiments. In this paper, we propose a novel approach by focusing on a set of target questions that are inherently more sensitive to jailbreak prompts, aiming to circumvent the limitations posed by enhanced LLM security. Through designing and analyzing these sensitive questions, this paper reveals a more effective method of identifying vulnerabilities in LLMs, thereby contributing to the advancement of LLM security. This research not only challenges existing jailbreaking methodologies but also fortifies LLMs against potential exploits.","Fri, 12 Apr 2024 08:08:44 UTC","이 논문은 새로운 접근 방식을 제안하여 민감한 질문에 초점을 맞추어 LLM의 취약성을 식별하는 더 효과적인 방법을 제시하고, 기존의 탈옥 방법론에 도전하며 LLM의 보안을 강화하는 데 기여한다.","['Large Language Models', 'Prompt Jailbreaking', 'research paradigm', 'attack strengths', 'sensitive questions', 'vulnerabilities', 'LLM security.']"
14,14,14,Pretraining and Updating Language- and Domain-specific Large Language Model: A Case Study in Japanese Business Domain,"['Kosuke Takahashi', 'Takahiro Omi', 'Kosuke Arima', 'Tatsuya Ishigaki']","Several previous studies have considered language- and domain-specific large language models (LLMs) as separate topics. This study explores the combination of a non-English language and a high-demand industry domain, focusing on a Japanese business-specific LLM. This type of a model requires expertise in the business domain, strong language skills, and regular updates of its knowledge. We trained a 13-billion-parameter LLM from scratch using a new dataset of business texts and patents, and continually pretrained it with the latest business documents. Further we propose a new benchmark for Japanese business domain question answering (QA) and evaluate our models on it. The results show that our pretrained model improves QA accuracy without losing general knowledge, and that continual pretraining enhances adaptation to new information. Our pretrained model and business domain benchmark are publicly available.","Fri, 12 Apr 2024 06:21:48 UTC",": Large language models, Japanese business-specific LLM, dataset, pretraining, question answering, benchmark.

한국어 요약: 이 연구는 일본 비즈니스 특화 LLM 모델을 새로운 데이터셋으로 훈련하고 최신 비즈니스 문서들로 지속적으로 사전 훈련함으로써 QA 정확도를 향상시키는 것을 제안하고, 이를 평가한 결과, 사전 훈련된 모델이 일반 지식을 잃지 않으면서 QA 정확도를 향상시키며, 지속적인 사전 훈련이 새로운 정보에 대한 적응을 향상시킨다는 것을 보여줍니다.","['Large language models', 'Japanese business-specific LLM', 'dataset', 'pretraining', 'question answering', 'benchmark.']"
15,15,15,Reducing hallucination in structured outputs via Retrieval-Augmented Generation,"['Patrice Béchard', 'Orlando Marquez Ayala']","A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.","Fri, 12 Apr 2024 01:42:09 UTC",": Generative AI, hallucinations, large language models, Retrieval Augmented Generation, structured output, workflows, generalization, retriever encoder, resource-intensive.

한국어 요약: 대규모 언어 모델을 활용하여 자연어 요구 사항을 기반으로 하는 엔터프라이즈 응용 프로그램을 배포하는 과정에서, Retrieval Augmented Generation(RAG)을 활용한 시스템을 개발하여 결과물의 환각을 크게 줄이고 일반화를 향상시켰으며, 작은, 잘 훈련된 리트리버 인코더를 사용하여 LLM 기반 시스템의 배포를 덜 리소스 소모적으로 만들 수 있음을 보여줌.","['Generative AI', 'hallucinations', 'large language models', 'Retrieval Augmented Generation', 'structured output', 'workflows', 'generalization', 'retriever encoder', 'resource-intensive.']"
16,16,16,Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs,"['Jierui Li', 'Raymond Mooney']","Distilling explicit chain-of-thought reasoning paths has emerged as an effective method for improving the reasoning abilities of large language models (LLMs) across various tasks. However, when tackling complex tasks that pose significant challenges for state-of-the-art models, this technique often struggles to produce effective chains of thought that lead to correct answers. In this work, we propose a novel approach to distill reasoning abilities from LLMs by leveraging their capacity to explain solutions. We apply our method to solving competitive-level programming challenges. More specifically, we employ an LLM to generate explanations for a set of <problem, solution-program> pairs, then use <problem, explanation> pairs to fine-tune a smaller language model, which we refer to as the Reasoner, to learn algorithmic reasoning that can generate ""how-to-solve"" hints for unseen problems. Our experiments demonstrate that learning from explanations enables the Reasoner to more effectively guide program implementation by a Coder, resulting in higher solve rates than strong chain-of-thought baselines on competitive-level programming problems. It also outperforms models that learn directly from <problem, solution-program> pairs. We curated an additional test set in the CodeContests format, which includes 246 more recent problems posted after the models' knowledge cutoff.","Thu, 11 Apr 2024 22:19:50 UTC",": reasoning paths, language models, distillation, competitive-level programming challenges, explanations, algorithmic reasoning, solve rates.

Summary (in Korean): 본 연구에서는 대규모 언어 모델의 추론 능력을 향상시키기 위해 해답을 설명하는 능력을 활용하여 ""어떻게 해결하는가"" 힌트를 생성하는 작은 언어 모델인 Reasoner를 훈련시키는 새로운 방법을 제안하였으며, 경쟁 수준의 프로그래밍 도전 과제에서 강력한 체인 오브 생각 기준을 능가하는 성능을 보였다.","['reasoning paths', 'language models', 'distillation', 'competitive-level programming challenges', 'explanations', 'algorithmic reasoning', 'solve rates.']"
17,17,17,LLM Agents can Autonomously Exploit One-day Vulnerabilities,"['Richard Fang', 'Rohan Bindu', 'Akul Gupta', 'Daniel Kang']","LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities.
In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.","Thu, 11 Apr 2024 22:07:19 UTC",": LLMs, cybersecurity vulnerabilities, autonomous hacking, one-day vulnerabilities, GPT-4.

Summary in Korean: 최근 연구에서 LLM 에이전트인 GPT-4가 CVE 설명을 통해 실제 시스템의 일일 취약점을 87% 정도 자동으로 악용할 수 있음을 보여주었으며, 이는 다른 모델 및 취약점 스캐너들과 비교하여 높은 성능을 보여주었으나, 설명 없이는 성능이 현저히 낮아짐을 발견하였다. 이로 인해 고도의 능력을 가진 LLM 에이전트의 광범위한 배포에 대한 문제가 제기되고 있다.","['LLMs', 'cybersecurity vulnerabilities', 'autonomous hacking', 'one-day vulnerabilities', 'GPT-4.']"
18,18,18,Generative Information Retrieval Evaluation,"['Marwah Alaofi', 'Negar Arabzadeh', 'Charles L. A. Clarke', 'Mark Sanderson']","In this chapter, we consider generative information retrieval evaluation from two distinct but interrelated perspectives. First, large language models (LLMs) themselves are rapidly becoming tools for evaluation, with current research indicating that LLMs may be superior to crowdsource workers and other paid assessors on basic relevance judgement tasks. We review past and ongoing related research, including speculation on the future of shared task initiatives, such as TREC, and a discussion on the continuing need for human assessments. Second, we consider the evaluation of emerging LLM-based generative information retrieval (GenIR) systems, including retrieval augmented generation (RAG) systems. We consider approaches that focus both on the end-to-end evaluation of GenIR systems and on the evaluation of a retrieval component as an element in a RAG system. Going forward, we expect the evaluation of GenIR systems to be at least partially based on LLM-based assessment, creating an apparent circularity, with a system seemingly evaluating its own output. We resolve this apparent circularity in two ways: 1) by viewing LLM-based assessment as a form of ""slow search"", where a slower IR system is used for evaluation and training of a faster production IR system; and 2) by recognizing a continuing need to ground evaluation in human assessment, even if the characteristics of that human assessment must change.","Thu, 11 Apr 2024 21:48:54 UTC","이 장에서는 대규모 언어 모델을 활용한 정보 검색 평가와 관련된 연구와 미래 전망, 인간 평가의 계속적인 필요성에 대해 다루며, 정보 검색 시스템의 평가와 인간 평가의 상호작용을 분석하고 있습니다.","['generative information retrieval evaluation', 'large language models (LLMs)', 'crowdsource workers', 'relevance judgement tasks', 'shared task initiatives', 'TREC', 'human assessments', 'generative information retrieval systems', 'retrieval augmented generation systems', 'end-to-end evaluation', 'LLM-based assessment', 'slow search', 'human assessment.']"
19,19,19,Auctions with LLM Summaries,"['Kumar Avinava Dubey', 'Zhe Feng', 'Rahul Kidambi', 'Aranyak Mehta', 'Di Wang']","We study an auction setting in which bidders bid for placement of their content within a summary generated by a large language model (LLM), e.g., an ad auction in which the display is a summary paragraph of multiple ads. This generalizes the classic ad settings such as position auctions to an LLM generated setting, which allows us to handle general display formats. We propose a novel factorized framework in which an auction module and an LLM module work together via a prediction model to provide welfare maximizing summary outputs in an incentive compatible manner. We provide a theoretical analysis of this framework and synthetic experiments to demonstrate the feasibility and validity of the system together with welfare comparisons.","Thu, 11 Apr 2024 21:05:56 UTC","본 연구는 입찰자들이 대형 언어 모델 (LLM)에 생성된 요약 내에서 자신의 콘텐츠 배치를 입찰하는 경매 설정을 연구하며, 입찰 모듈과 LLM 모듈이 예측 모델을 통해 협력하여 인센티브 호환적인 방식으로 최대 복지를 제공하는 요약 출력을 제공하는 새로운 요소화된 프레임워크를 제안합니다.","['auction setting', 'bidders', 'content placement', 'large language model (LLM)', 'ad auction', 'display formats', 'factorized framework', 'prediction model', 'welfare maximizing summary outputs', 'incentive compatible', 'theoretical analysis', 'synthetic experiments', 'welfare comparisons.']"
20,20,20,Data-Augmentation-Based Dialectal Adaptation for LLMs,"['Fahim Faisal', 'Antonios Anastasopoulos']","This report presents GMUNLP's participation to the Dialect-Copa shared task at VarDial 2024, which focuses on evaluating the commonsense reasoning capabilities of large language models (LLMs) on South Slavic micro-dialects. The task aims to assess how well LLMs can handle non-standard dialectal varieties, as their performance on standard languages is already well-established. We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects: Chakavian, Cherkano, and Torlak. We conduct experiments using a language-family-focused encoder-based model (BERTić) and a domain-agnostic multilingual model (AYA-101). Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open-source model category. This work highlights the practical utility of data augmentation and the potential of LLMs in handling non-standard dialectal varieties, contributing to the broader goal of advancing natural language understanding in low-resource and dialectal settings. Code:this https URL","Thu, 11 Apr 2024 19:15:32 UTC",2024년 VarDial에서 진행된 Dialect-Copa 공유 작업에 참여한 GMUNLP는 남슬라브 소토-다이아렉트에 대한 일반적인 상식 추론 능력을 평가하는 작업에서 데이터 증강 기술을 활용하여 성능 향상을 이룩한 것을 보여주었습니다.,"['GMUNLP', 'Dialect-Copa shared task', 'VarDial 2024', 'large language models', 'South Slavic micro-dialects', 'data augmentation techniques', 'Chakavian', 'Cherkano', 'Torlak', 'BERTić', 'AYA-101', 'performance gains.']"
21,21,21,SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions,"['Stefan Sylvius Wagner', 'Maike Behrendt', 'Marc Ziegele', 'Stefan Harmeling']","Stance detection is an important task for many applications that analyse or support online political discussions. Common approaches include fine-tuning transformer based models. However, these models require a large amount of labelled data, which might not be available. In this work, we present two different ways to leverage LLM-generated synthetic data to train and improve stance detection agents for online political discussions: first, we show that augmenting a small fine-tuning dataset with synthetic data can improve the performance of the stance detection model. Second, we propose a new active learning method called SQBC based on the ""Query-by-Comittee"" approach. The key idea is to use LLM-generated synthetic data as an oracle to identify the most informative unlabelled samples, that are selected for manual labelling. Comprehensive experiments show that both ideas can improve the stance detection performance. Curiously, we observed that fine-tuning on actively selected samples can exceed the performance of using the full dataset.","Thu, 11 Apr 2024 18:34:11 UTC",": Stance detection, transformer based models, synthetic data, active learning, Query-by-Committee, LLM-generated data.

한국어 요약: 온라인 정치 토론을 분석하거나 지원하는 많은 응용 프로그램에 중요한 과제인 스탠스 감지를 개선하기 위해 LLM 생성 합성 데이터를 활용하는 두 가지 방법을 제안하고, 작은 미세 조정 데이터 세트를 합성 데이터로 보강하면 성능이 향상되며, SQBC라는 새로운 액티브 러닝 방법을 제안하여 성능을 향상시킬 수 있음을 실험을 통해 입증함.","['Stance detection', 'transformer based models', 'synthetic data', 'active learning', 'Query-by-Committee', 'LLM-generated data.']"
22,22,22,MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference,"['Mobashir Sadat', 'Cornelia Caragea']","The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.","Thu, 11 Apr 2024 18:12:12 UTC",": scientific Natural Language Inference (NLI), SciNLI, MSciNLI, dataset, domains, Pre-trained Language Models (PLMs), Large Language Models (LLMs, domain shift, transfer learning.

한국어 요약: 과학적 자연어 추론 작업에 다양성을 도입하고, 다양한 과학 분야에서 추출한 132,320개의 문장 쌍을 포함하는 MSciNLI 데이터셋을 소개하며, 이를 통해 과학 NLI 모델의 성능을 향상시킬 수 있음을 보여준 논문입니다.","['scientific Natural Language Inference (NLI)', 'SciNLI', 'MSciNLI', 'dataset', 'domains', 'Pre-trained Language Models (PLMs)', 'Large Language Models (LLMs', 'domain shift', 'transfer learning.']"
23,23,23,A Multi-Expert Large Language Model Architecture for Verilog Code Generation,"['Bardia Nadimi', 'Hao Zheng']","Recently, there has been a surging interest in using large language models (LLMs) for Verilog code generation. However, the existing approaches are limited in terms of the quality of the generated Verilog code. To address such limitations, this paper introduces an innovative multi-expert LLM architecture for Verilog code generation (MEV-LLM). Our architecture uniquely integrates multiple LLMs, each specifically fine-tuned with a dataset that is categorized with respect to a distinct level of design complexity. It allows more targeted learning, directly addressing the nuances of generating Verilog code for each category. Empirical evidence from experiments highlights notable improvements in terms of the percentage of generated Verilog outputs that are syntactically and functionally correct. These findings underscore the efficacy of our approach, promising a forward leap in the field of automated hardware design through machine learning.","Thu, 11 Apr 2024 16:58:29 UTC",": large language models, Verilog code generation, multi-expert LLM architecture, dataset, design complexity, syntactically and functionally correct.

한줄 요약: 본 논문에서는 Verilog 코드 생성을 위한 혁신적인 다중 전문가 LLM 아키텍처를 소개하며, 다양한 수준의 디자인 복잡성에 따라 세분화된 데이터셋으로 특별히 세밀하게 조정된 여러 LLM을 통합하여 Verilog 코드 생성에 대한 뉘앙스를 직접적으로 처리함으로써 효율적인 결과를 도출하였습니다.","['large language models', 'Verilog code generation', 'multi-expert LLM architecture', 'dataset', 'design complexity', 'syntactically and functionally correct.']"
24,24,24,Analyzing the Performance of Large Language Models on Code Summarization,"['Rajarshi Haldar', 'Julia Hockenmaier']","Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance of these models on individual examples often depends on the amount of (subword) token overlap between the code and the corresponding reference natural language descriptions in the dataset. This token overlap arises because the reference descriptions in standard datasets (corresponding to docstrings in large code bases) are often highly similar to the names of the functions they describe. We also show that this token overlap occurs largely in the function names of the code and compare the relative performance of these models after removing function names versus removing code structure. We also show that using multiple evaluation metrics like BLEU and BERTScore gives us very little additional insight since these metrics are highly correlated with each other.","Wed, 10 Apr 2024 22:42:18 UTC","Llama 2와 같은 대형 언어 모델은 코드 요약 작업에서 성능이 뛰어나며, 코드와 자연어 설명 사이의 토큰 중복이 중요한 역할을 함을 보여줌으로써, 다양한 평가 지표들이 서로 상관관계가 높다는 것을 보여줌.","['Large language models', 'Llama 2', 'code summarization', 'token overlap', 'function names', 'evaluation metrics', 'BLEU', 'BERTScore.']"
25,25,25,Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition,"['Kehua Feng', 'Keyan Ding', 'Kede Ma', 'Zhihua Wang', 'Qiang Zhang', 'Huajun Chen']","The past years have witnessed a proliferation of large language models (LLMs). Yet, automated and unbiased evaluation of LLMs is challenging due to the inaccuracy of standard metrics in reflecting human preferences and the inefficiency in sampling informative and diverse test examples. While human evaluation remains the gold standard, it is expensive and time-consuming, especially when dealing with a large number of testing samples. To address this problem, we propose a sample-efficient human evaluation method based on MAximum Discrepancy (MAD) competition. MAD automatically selects a small set of informative and diverse instructions, each adapted to two LLMs, whose responses are subject to three-alternative forced choice by human subjects. The pairwise comparison results are then aggregated into a global ranking using the Elo rating system. We select eight representative LLMs and compare them in terms of four skills: knowledge understanding, mathematical reasoning, writing, and coding. Experimental results show that the proposed method achieves a reliable and sensible ranking of LLMs' capabilities, identifies their relative strengths and weaknesses, and offers valuable insights for further LLM advancement.","Wed, 10 Apr 2024 01:26:24 UTC",": large language models, automated evaluation, unbiased evaluation, human evaluation, MAximum Discrepancy (MAD) competition, Elo rating system, knowledge understanding, mathematical reasoning, writing, coding.

한국어 요약: 대규모 언어 모델(Large Language Models, LLMs)의 폭발적인 증가에도 불구하고, 자동화된 그리고 편향되지 않은 LLMs의 평가는 인간의 선호도를 반영하는 표준 지표의 부정확성과 정보적이고 다양한 테스트 예제를 샘플링하는 효율성의 한계로 어려움을 겪고 있습니다.","['large language models', 'automated evaluation', 'unbiased evaluation', 'human evaluation', 'MAximum Discrepancy (MAD) competition', 'Elo rating system', 'knowledge understanding', 'mathematical reasoning', 'writing', 'coding.']"
26,26,26,Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics,"['Zhengde Zhang', 'Yiyu Zhang', 'Haodong Yao', 'Jianwen Luo', 'Rui Zhao', 'Bo Huang', 'Jiameng Zhao', 'Yipu Liao', 'Ke Li', 'Lina Zhao', 'Jun Cao', 'Fazhi Qi', 'Changzheng Yuan']","Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific field, it's challenging to acquire unique domain knowledge while keeping the model itself advanced. To address this challenge, a sophisticated large language model system named as Xiwu has been developed, allowing you switch between the most advanced foundation models and quickly teach the model domain knowledge. In this work, we will report on the best practices for applying LLMs in the field of high-energy physics (HEP), including: a seed fission technology is proposed and some data collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system has been developed to facilitate rapid training under a specified foundation model. The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna, ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model on the HEP knowledge question-and-answering and code generation. This strategy significantly enhances the potential for growth of our model's performance, with the hope of surpassing GPT-4 as it evolves with the development of open-source models. This work provides a customized LLM for the field of HEP, while also offering references for applying LLM to other fields, the corresponding codes are available on Github.","Mon, 8 Apr 2024 07:37:31 UTC ",": Large Language Models, Xiwu, high-energy physics, AI-Ready dataset, just-in-time learning, fine-tuning, benchmark model, GPT-4, open-source models, Github.

한국어 요약: 대형 언어 모델은 빠르게 업데이트되고 변화하고 있으며, 특정 과학 분야에 적용할 때 도메인 지식을 획득하면서 모델 자체를 최신 상태로 유지하는 것이 어려운데, 이를 해결하기 위해 Xiwu라는 고도로 발전된 대형 언어 모델 시스템이 개발되었으며, 이를 통해 고에너지 물리학 분야에 적용하는데 있어서 가장 좋은 방법론을 보고하고 있습니다.","['Large Language Models', 'Xiwu', 'high-energy physics', 'AI-Ready dataset', 'just-in-time learning', 'fine-tuning', 'benchmark model', 'GPT-4', 'open-source models', 'Github.']"
27,27,27,OpenBias: Open-set Bias Detection in Text-to-Image Generative Models,"[""Moreno D'Incà"", 'Elia Peruzzo', 'Massimiliano Mancini', 'Dejia Xu', 'Vidit Goel', 'Xingqian Xu', 'Zhangyang Wang', 'Humphrey Shi', 'Nicu Sebe']","Text-to-image generative models are becoming increasingly popular and accessible to the general public. As these models see large-scale deployments, it is necessary to deeply investigate their safety and fairness to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in text-to-image generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a Large Language Model (LLM) to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision Question Answering model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.","Thu, 11 Apr 2024 17:59:56 UTC",": Text-to-image generative models, safety, fairness, biases, OpenBias, Large Language Model (LLM), Vision Question Answering model, Stable Diffusion.

한국어 요약: 텍스트-이미지 생성 모델의 안전성과 공정성을 탐구하기 위해 OpenBias라는 새로운 파이프라인을 제안하고, 이를 통해 새로운 편향을 객관적으로 식별하고 측정하는 연구를 수행하였습니다.","['Text-to-image generative models', 'safety', 'fairness', 'biases', 'OpenBias', 'Large Language Model (LLM)', 'Vision Question Answering model', 'Stable Diffusion.']"
28,28,28,Manipulating Large Language Models to Increase Product Visibility,"['Aounon Kumar', 'Himabindu Lakkaraju']","Large language models (LLMs) are increasingly being integrated into search engines to provide natural language responses tailored to user queries. Customers and end-users are also becoming more dependent on these models for quick and easy purchase decisions. In this work, we investigate whether recommendations from LLMs can be manipulated to enhance a product's visibility. We demonstrate that adding a strategic text sequence (STS) -- a carefully crafted message -- to a product's information page can significantly increase its likelihood of being listed as the LLM's top recommendation. To understand the impact of STS, we use a catalog of fictitious coffee machines and analyze its effect on two target products: one that seldom appears in the LLM's recommendations and another that usually ranks second. We observe that the strategic text sequence significantly enhances the visibility of both products by increasing their chances of appearing as the top recommendation. This ability to manipulate LLM-generated search responses provides vendors with a considerable competitive advantage and has the potential to disrupt fair market competition. Just as search engine optimization (SEO) revolutionized how webpages are customized to rank higher in search engine results, influencing LLM recommendations could profoundly impact content optimization for AI-driven search services. Code for our experiments is available at this https URL.","Thu, 11 Apr 2024 17:57:32 UTC",": Large language models, search engines, product visibility, strategic text sequence, recommendations, competitive advantage, search engine optimization, AI-driven search services.

한국어 요약: 대형 언어 모델(Large language models, LLMs)이 검색 엔진에 통합되어 사용자 쿼리에 맞춘 자연어 응답을 제공하는데 사용되고 있으며, 제품의 가시성을 향상시키기 위해 LLM의 권장 사항을 조작할 수 있는지 조사하는 연구입니다.","['Large language models', 'search engines', 'product visibility', 'strategic text sequence', 'recommendations', 'competitive advantage', 'search engine optimization', 'AI-driven search services.']"
29,29,29,LLoCO: Learning Long Contexts Offline,"['Sijun Tan', 'Xiuyu Li', 'Shishir Patil', 'Ziyang Wu', 'Tianjun Zhang', 'Kurt Keutzer', 'Joseph E. Gonzalez', 'Raluca Ada Popa']","Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. We propose a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning. Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. We introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient finetuning using LoRA. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using 30\times fewer tokens during inference. LLoCO achieves up to 7.62\times speed-up and substantially reduces the cost of long document question answering, making it a promising solution for efficient long context processing. Our code is publicly available at this https URL.","Thu, 11 Apr 2024 17:57:22 UTC",": large language models, context compression, parameter-efficient finetuning, LLoCO, long-context question-answering datasets, inference

요약: LLoCO는 컨텍스트 압축, 검색 및 매개변수 효율적인 미세조정을 결합하여 LLM이 원래 컨텍스트의 간결한 표현을 만들고 질문에 정확하게 답변하기 위해 관련 정보를 효율적으로 검색할 수 있도록 하는 혁신적인 접근 방식으로, LLaMA2-7B 모델의 효과적인 컨텍스트 창을 128k 토큰까지 처리할 수 있도록 확장하며, 30배 적은 토큰을 사용하여 추론 중에 뛰어난 성능을 보이며, 비용을 크게 줄여 효율적인 장문 컨","['large language models', 'context compression', 'parameter-efficient finetuning', 'LLoCO', 'long-context question-answering datasets', 'inference']"
30,30,30,Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models,"['Haotian Zhang', 'Haoxuan You', 'Philipp Dufter', 'Bowen Zhang', 'Chen Chen', 'Hong-You Chen', 'Tsu-Jui Fu', 'William Yang Wang', 'Shih-Fu Chang', 'Zhe Gan', 'Yinfei Yang']","While Ferret seamlessly integrates regional understanding into the Large Language Model (LLM) to facilitate its referring and grounding capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks. In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs. (1) Any resolution grounding and referring: A flexible approach that effortlessly handles higher image resolution, improving the model's ability to process and understand images in greater detail. (2) Multi-granularity visual encoding: By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information. (3) A three-stage training paradigm: Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final instruction tuning. Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing.","Thu, 11 Apr 2024 17:56:05 UTC",": Ferret, Large Language Model (LLM), Ferret-v2, regional understanding, visual encoder, image resolution, visual encoding, three-stage training paradigm, high-resolution scaling, fine-grained visual processing

한국어 요약: Ferret-v2는 Ferret의 중요한 업그레이드로, 고해상도 처리와 세밀한 시각적 처리를 통해 Ferret 및 최신 기술들보다 상당한 개선을 제공하며 이미지를 더 자세히 처리하고 이해하는 모델의 능력을 향상시킵니다.","['Ferret', 'Large Language Model (LLM)', 'Ferret-v2', 'regional understanding', 'visual encoder', 'image resolution', 'visual encoding', 'three-stage training paradigm', 'high-resolution scaling', 'fine-grained visual processing']"
31,31,31,OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments,"['Tianbao Xie', 'Danyang Zhang', 'Jixuan Chen', 'Xiaochuan Li', 'Siheng Zhao', 'Ruisheng Cao', 'Toh Jing Hua', 'Zhoujun Cheng', 'Dongchan Shin', 'Fangyu Lei', 'Yitao Liu', 'Yiheng Xu', 'Shuyan Zhou', 'Silvio Savarese', 'Caiming Xiong', 'Victor Zhong', 'Tao Yu']","Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.","Thu, 11 Apr 2024 17:56:05 UTC",": Autonomous agents, human-computer interaction, benchmarks, OSWorld, multimodal agents, operating systems, computer tasks, computer use cases, evaluation.

한국어 요약: OSWorld는 Ubuntu, Windows, macOS 등 다양한 운영 체제에서 작업 설정, 실행 기반 평가, 상호 작용 학습을 지원하는 첫 번째 실제 컴퓨터 환경으로, 실제 웹 및 데스크톱 앱, 운영 체제 파일 I/O, 다중 애플리케이션을 포함한 369가지 컴퓨터 작업에 대한 벤치마크를 만들어 상호 작용적인 컴퓨터 작업을 평가하는 통합된 컴퓨터 환경으로 활용될 수 있음을 보여줍니다.","['Autonomous agents', 'human-computer interaction', 'benchmarks', 'OSWorld', 'multimodal agents', 'operating systems', 'computer tasks', 'computer use cases', 'evaluation.']"
32,32,32,EduAgent: Generative Student Agents in Learning,"['Songlin Xu', 'Xinyu Zhang', 'Lianhui Qin']","Student simulation in online education is important to address dynamic learning behaviors of students with diverse backgrounds. Existing simulation models based on deep learning usually need massive training data, lacking prior knowledge in educational contexts. Large language models (LLMs) may contain such prior knowledge since they are pre-trained from a large corpus. However, because student behaviors are dynamic and multifaceted with individual differences, directly prompting LLMs is not robust nor accurate enough to capture fine-grained interactions among diverse student personas, learning behaviors, and learning outcomes. This work tackles this problem by presenting a newly annotated fine-grained large-scale dataset and proposing EduAgent, a novel generative agent framework incorporating cognitive prior knowledge (i.e., theoretical findings revealed in cognitive science) to guide LLMs to first reason correlations among various behaviors and then make simulations. Our two experiments show that EduAgent could not only mimic and predict learning behaviors of real students but also generate realistic learning behaviors of virtual students without real data.","Sat, 23 Mar 2024 18:19:17 UTC","다양한 학생들의 동적 학습 행동을 다루기 위해 새로운 세부적인 대규모 데이터셋을 제시하고, 인지 과학에서 밝혀진 이론적 결과를 활용하여 대규모 언어 모델을 이끌어 다양한 행동들 간의 상관관계를 이해하고 시뮬레이션을 만들어내는 새로운 생성 에이전트 프레임워크인 EduAgent를 제안한 연구입니다.","['Student simulation', 'online education', 'deep learning', 'large language models', 'fine-grained interactions', 'diverse student personas', 'cognitive prior knowledge', 'generative agent framework', 'learning behaviors.']"
33,33,33,Content Knowledge Identification with Multi-Agent Large Language Models (LLMs),"['Kaiqi Yang', 'Yucheng Chu', 'Taylor Darwin', 'Ahreum Han', 'Hang Li', 'Hongzhi Wen', 'Yasemin Copur-Gencturk', 'Jiliang Tang', 'Hui Liu']","Teachers' mathematical content knowledge (CK) is of vital importance and need in teacher professional development (PD) programs. Computer-aided asynchronous PD systems are the most recent proposed PD techniques, which aim to help teachers improve their PD equally with fewer concerns about costs and limitations of time or location. However, current automatic CK identification methods, which serve as one of the core techniques of asynchronous PD systems, face challenges such as diversity of user responses, scarcity of high-quality annotated data, and low interpretability of the predictions. To tackle these challenges, we propose a Multi-Agent LLMs-based framework, LLMAgent-CK, to assess the user responses' coverage of identified CK learning goals without human annotations. By taking advantage of multi-agent LLMs in strong generalization ability and human-like discussions, our proposed LLMAgent-CK presents promising CK identifying performance on a real-world mathematical CK dataset MaCKT. Moreover, our case studies further demonstrate the working of the multi-agent framework.","Fri, 22 Mar 2024 02:37:33 UTC",": Teachers, mathematical content knowledge, teacher professional development, computer-aided asynchronous PD systems, CK identification methods, Multi-Agent LLMs-based framework, LLMAgent-CK, user responses, CK learning goals, real-world mathematical CK dataset MaCKT.

한줄 요약: 교사들의 수학 콘텐츠 지식은 교사 전문 개발 프로그램에서 중요한 역할을 하며, 다양한 사용자 응답과 고품질 주석 데이터 부족 등의 도전에 직면한 현재의 자동 CK 식별 방법을 극복하기 위해 Multi-Agent LLMs 기반 프레임워크인 LLMAgent-CK를 제안하고 있습니다.","['Teachers', 'mathematical content knowledge', 'teacher professional development', 'computer-aided asynchronous PD systems', 'CK identification methods', 'Multi-Agent LLMs-based framework', 'LLMAgent-CK', 'user responses', 'CK learning goals', 'real-world mathematical CK dataset MaCKT.']"
34,34,34,ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference,"['Hyungjun Oh', 'Kihong Kim', 'Jaemin Kim', 'Sungkyun Kim', 'Junyeol Lee', 'Du-seong Chang', 'Jiwon Seo']","This paper presents ExeGPT, a distributed system designed for constraint-aware LLM inference. ExeGPT finds and runs with an optimal execution schedule to maximize inference throughput while satisfying a given latency constraint. By leveraging the distribution of input and output sequences, it effectively allocates resources and determines optimal execution configurations, including batch sizes and partial tensor parallelism. We also introduce two scheduling strategies based on Round-Robin Allocation and Workload-Aware Allocation policies, suitable for different NLP workloads. We evaluate ExeGPT on six LLM instances of T5, OPT, and GPT-3 and five NLP tasks, each with four distinct latency constraints. Compared to FasterTransformer, ExeGPT achieves up to 15.2x improvements in throughput and 6x improvements in latency. Overall, ExeGPT achieves an average throughput gain of 2.9x across twenty evaluation scenarios. Moreover, when adapting to changing sequence distributions, the cost of adjusting the schedule in ExeGPT is reasonably modest. ExeGPT proves to be an effective solution for optimizing and executing LLM inference for diverse NLP workload and serving conditions.","Fri, 15 Mar 2024 06:21:56 UTC","ExeGPT는 분산 시스템으로, 제약 조건을 고려한 LLM 추론을 최적화하기 위해 실행 일정을 찾고 실행하여 추론 처리량을 최대화하며 주어진 지연 제약 조건을 충족시키는데 도움을 줍니다.","['ExeGPT', 'distributed system', 'constraint-aware LLM inference', 'optimal execution schedule', 'throughput', 'latency constraint', 'resource allocation', 'scheduling strategies', 'NLP workloads', 'evaluation scenarios.']"
35,35,35,InfiCoder-Eval: Systematically Evaluating the Question-Answering Capabilities of Code Large Language Models,"['Linyi Li', 'Shijie Geng', 'Zhenwen Li', 'Yibo He', 'Hao Yu', 'Ziyue Hua', 'Guanghan Ning', 'Siwei Wang', 'Tao Xie', 'Hongxia Yang']","Large Language Models for understanding and generating code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiCoder-Eval, a large-scale freeform question-answering (QA) benchmark for code, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. To evaluate the response correctness, InfiCoder-Eval supports four types of model-free metrics and domain experts carefully choose and concretize the criterion for each question. We conduct a systematic evaluation for more than 80 code LLMs on InfiCoder-Eval, leading to a series of insightful findings. Furthermore, our detailed analyses showcase possible directions for further improvement of code LLMs. InfiCoder-Eval is fully open source at this https URL and continuously maintaining and expanding to foster more scientific and systematic practices for evaluating code LLMs.","Mon, 11 Mar 2024 02:06:30 UTC",": Large Language Models, code understanding, code generation tasks, evaluation benchmarks, InfiCoder-Eval, question-answering benchmark, Stack Overflow questions, programming languages, model-free metrics, systematic evaluation, code LLMs improvement.

Summary in Korean: 최근 몇 년간 코드 이해 및 생성을 위한 대규모 언어 모델인 코드 LLMs가 엄청난 발전을 이루었으며, 이에 대한 성능 측정을 위해 InfiCoder-Eval이라는 대규모 자유형 질문응답(QA) 벤치마크가 제안되었으며, 이를 통해 코드 LLMs의 평가 및 개선 가능성에 대한 연구가 이루어지고 있습니다.","['Large Language Models', 'code understanding', 'code generation tasks', 'evaluation benchmarks', 'InfiCoder-Eval', 'question-answering benchmark', 'Stack Overflow questions', 'programming languages', 'model-free metrics', 'systematic evaluation', 'code LLMs improvement.']"
36,36,36,Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation,"['Jinkyung Park', 'Pamela Wisniewski', 'Vivek Singh']","In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized. Therefore, we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI community's methods for working with data.","Thu, 11 Apr 2024 17:20:57 UTC",": LLMs, interactive research tools, collaboration, human-AI labeling, online risk data, data annotation, co-labeling tasks, contextualized data, HCI research community.

Summary in Korean: 이 포지션 논문에서는 LLMs를 상호작용적인 연구 도구로 활용하여 인간 코더와 AI 간 협력을 촉진하여 온라인 위험 데이터를 효과적으로 주석 처리하는 잠재력에 대해 논의하며, 온라인 위험에 대한 문맥화된 데이터 주석을 위한 LLMs 기반 도구의 이점과 도전에 대해 제시하고 HCI 연구 커뮤니티에게 LLMs를 연구 도구로 활용하여 인간-AI 협력을 촉진하는 방향을 제안합니다.","['LLMs', 'interactive research tools', 'collaboration', 'human-AI labeling', 'online risk data', 'data annotation', 'co-labeling tasks', 'contextualized data', 'HCI research community.']"
37,37,37,LaVy: Vietnamese Multimodal Large Language Model,"['Chi Tran', 'Huong Le Thanh']","Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. All code and model weights are public at this https URL","Thu, 11 Apr 2024 17:09:28 UTC",": Large Language Models, Multimodal Large Language Models, Vietnamese, LaVy, MLLMs, benchmark.

한줄 요약: 베트남어 다중언어 대형 언어 모델에 대한 높은 품질의 리소스 부족으로 인해 발전이 제한되었으나, 이 논문에서는 LaVy라는 베트남어 다중언어 대형 언어 모델을 소개하고, 베트남어 시각 언어 작업에 대한 MLLMs의 이해를 평가하기 위한 벤치마크인 LaVy-Bench를 소개합니다.","['Large Language Models', 'Multimodal Large Language Models', 'Vietnamese', 'LaVy', 'MLLMs', 'benchmark.']"
38,38,38,AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs,"['Zeyi Liao', 'Huan Sun']","As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100\% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.","Thu, 11 Apr 2024 17:05:50 UTC","대형 언어 모델의 안전을 보장하는 것이 중요한 가운데, GCG 최적화를 통해 적대적 접미사를 생성하는 새로운 방법을 소개한 연구에서, 적대적 접미사를 생성하는 일반적이고 전이 가능한 생성 모델인 AmpleGCG를 제안하고 성공적인 공격 성공률을 달성했다.","['large language models', 'safety', 'GCG optimization', 'adversarial suffixes', 'generative model', 'attack success rate', 'transferability.']"
39,39,39,High-Dimension Human Value Representation in Large Language Models,"['Samuel Cahyawijaya', 'Delong Chen', 'Yejin Bang', 'Leila Khalatbari', 'Bryan Wilie', 'Ziwei Ji', 'Etsuko Ishii', 'Pascale Fung']","The widespread application of Large Language Models (LLMs) across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from Reinforcement Learning with Human Feedback (RLHF), to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly large scale human annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in LLMs, orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual LLMs and tested on the output from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different LLMs with different langauge sources. Through UniVaR, we explore how different LLMs prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.","Thu, 11 Apr 2024 16:39:00 UTC",": Large Language Models (LLMs), human values, UniVaR, multilingual LLMs, language sources, human values alignment.

한국어 요약: 다양한 작업과 분야에서 대규모 언어 모델 (LLMs)의 광범위한 응용은 이러한 모델을 인간의 가치와 선호도에 맞추는 필요성을 제기하고 있습니다. UniVaR이라는 고차원 표현을 제안하여 다양한 LLMs에 내장된 인간 가치의 분포를 비교하는 강력한 도구임을 보여주며, 이를 통해 다양한 언어와 문화에서 다른 가치를 우선시하는 LLMs의 복잡한 상호작용을 밝히고 있습니다.","['Large Language Models (LLMs)', 'human values', 'UniVaR', 'multilingual LLMs', 'language sources', 'human values alignment.']"
40,40,40,Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations,"['Dayeon Ki', 'Marine Carpuat']","Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.","Thu, 11 Apr 2024 15:47:10 UTC","번역, 대형 언어 모델, 지도 학습 시스템, 품질 측정, 후편집, 피드백, 중국어-영어, 영어-독일어, 영어-러시아어.

요약: 이 연구는 대형 언어 모델과 지도 학습 기계 번역 시스템의 보완적인 강점을 활용하여, 외부 피드백을 통해 LLMs가 MT를 자동으로 후편집하도록 유도하는 방법을 제시하고, 중국어-영어, 영어-독일어, 영어-러시아어 MQM 데이터를 활용한 실험을 통해 번역 품질을 향상시킬 수 있음을 입증하였습니다.","['역', '대형 언어 모델', '지도 학습 시스템', '품질 측정', '후편집', '피드백', '중국어-영어', '영어-독일어', '영어-러시아어.']"
41,41,41,Post-Hoc Reversal: Are We Selecting Models Prematurely?,"['Rishabh Ranjan', 'Saurabh Garg', 'Mrigank Raman', 'Carlos Guestrin', 'Zachary Chase Lipton']","Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc. However, such transforms are typically applied only after the base models have already been finalized by standard means. In this paper, we challenge this practice with an extensive empirical study. In particular, we demonstrate a phenomenon that we call post-hoc reversal, where performance trends are reversed after applying these post-hoc transforms. This phenomenon is especially prominent in high-noise settings. For example, while base models overfit badly early in training, both conventional ensembling and SWA favor base models trained for more epochs. Post-hoc reversal can also suppress the appearance of double descent and mitigate mismatches between test loss and test error seen in base models. Based on our findings, we propose post-hoc selection, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices. Our experimental analyses span real-world vision, language, tabular and graph datasets from domains like satellite imaging, language modeling, census prediction and social network analysis. On an LLM instruction tuning dataset, post-hoc selection results in > 1.5x MMLU improvement compared to naive selection. Code is available at this https URL.","Thu, 11 Apr 2024 14:58:19 UTC","이 논문에서는 훈련된 모델들이 성능, 견고성, 불확실성 추정 등을 향상시키기 위해 후처리 변환 기법을 적용하는데, 이러한 변환들이 종종 기존 모델의 성능 추이를 뒤집는 현상을 발견하고 이를 바탕으로 후처리 선택 기법을 제안하였다.","['trained models', 'post-hoc transforms', 'temperature scaling', 'ensembling', 'stochastic weight averaging', 'performance', 'uncertainty estimation', 'post-hoc reversal', 'high-noise settings', 'double descent', 'post-hoc selection', 'model development decisions', 'hyperparameter choices', 'experimental analyses.']"
42,42,42,Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation,"['Stephen Bothwell', 'Abigail Swenor', 'David Chiang']","This paper describes submissions from the team Nostra Domina to the EvaLatin 2024 shared task of emotion polarity detection. Given the low-resource environment of Latin and the complexity of sentiment in rhetorical genres like poetry, we augmented the available data through automatic polarity annotation. We present two methods for doing so on the basis of the k
-means algorithm, and we employ a variety of Latin large language models (LLMs) in a neural architecture to better capture the underlying contextual sentiment representations. Our best approach achieved the second highest macro-averaged Macro-F_1
score on the shared task's test set.","Thu, 11 Apr 2024 14:35:23 UTC",": Nostra Domina, EvaLatin 2024, emotion polarity detection, Latin, sentiment, poetry, data augmentation, k-means algorithm, Latin large language models (LLMs), neural architecture, Macro-F1 score.

한국어 요약: 논문에서는 라틴어의 감정 극성 감지에 대한 Nostra Domina 팀의 제출물에 대해 설명하며, 라틴어의 저자원 환경과 시의적 장르인 시에서의 감정의 복잡성을 고려하여 사용 가능한 데이터를 자동 극성 주석을 통해 보강하였고, k-means 알고리즘을 기반으로 이를 수행하는 두 가지 방법을 제시하였으며, 라틴어 대형 언어 모델 (LLMs)을 다양하게 활용하여 신경 아키텍처에서 기반이 되는 문맥적","['Nostra Domina', 'EvaLatin 2024', 'emotion polarity detection', 'Latin', 'sentiment', 'poetry', 'data augmentation', 'k-means algorithm', 'Latin large language models (LLMs)', 'neural architecture', 'Macro-F1 score.']"
43,43,43,Discourse-Aware In-Context Learning for Temporal Expression Normalization,"['Akash Kumar Gautam', 'Lukas Lange', 'Jannik Strötgen']","Temporal expression (TE) normalization is a well-studied problem. However, the predominately used rule-based systems are highly restricted to specific settings, and upcoming machine learning approaches suffer from a lack of labeled data. In this work, we explore the feasibility of proprietary and open-source large language models (LLMs) for TE normalization using in-context learning to inject task, document, and example information into the model. We explore various sample selection strategies to retrieve the most relevant set of examples. By using a window-based prompt design approach, we can perform TE normalization across sentences, while leveraging the LLM knowledge without training the model. Our experiments show competitive results to models designed for this task. In particular, our method achieves large performance improvements for non-standard settings by dynamically including relevant examples during inference.","Thu, 11 Apr 2024 14:13:44 UTC","이 연구는 특정 설정에 제한된 규칙 기반 시스템과 labeled data 부족으로 인한 기계 학습 접근 방식의 한계를 극복하기 위해, TE normalization에 대한 대규모 언어 모델을 활용한 새로운 방법을 탐구하고 있으며, 이를 통해 경쟁력 있는 결과를 얻고 있다.","['Temporal expression normalization', 'rule-based systems', 'machine learning approaches', 'large language models', 'in-context learning', 'sample selection strategies', 'window-based prompt design', 'competitive results.']"
44,44,44,Generating consistent PDDL domains with Large Language Models,"['Pavel Smirnov', 'Frank Joublin', 'Antonello Ceravola', 'Michael Gienger']","Large Language Models (LLMs) are capable of transforming natural language domain descriptions into plausibly looking PDDL markup. However, ensuring that actions are consistent within domains still remains a challenging task. In this paper we present a novel concept to significantly improve the quality of LLM-generated PDDL models by performing automated consistency checking during the generation process. Although the proposed consistency checking strategies still can't guarantee absolute correctness of generated models, they can serve as valuable source of feedback reducing the amount of correction efforts expected from a human in the loop. We demonstrate the capabilities of our error detection approach on a number of classical and custom planning domains (logistics, gripper, tyreworld, household, pizza).","Thu, 11 Apr 2024 13:48:48 UTC",": Large Language Models, PDDL markup, consistency checking, error detection, planning domains.
Summary in Korean: 대형 언어 모델은 PDDL 마크업으로 변환하는 능력을 가지고 있지만 도메인 내에서 일관성을 유지하는 것은 여전히 어려운 과제입니다. 본 논문에서는 LLM이 생성하는 PDDL 모델의 품질을 획기적으로 향상시키기 위해 생성 과정 중 자동 일관성 검사를 수행하는 새로운 개념을 제시합니다.","['Large Language Models', 'PDDL markup', 'consistency checking', 'error detection', 'planning domains.\nSummary in Korean: 대형 언어 모델은 PDDL 마크업으로 변환하는 능력을 가지고 있지만 도메인 내에서 일관성을 유지하는 것은 여전히 어려운 과제입니다. 본 논문에서는 LLM이 생성하는 PDDL 모델의 품질을 획기적으로 향상시키기 위해 생성 과정 중 자동 일관성 검사를 수행하는 새로운 개념을 제시합니다']"
45,45,45,Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models,"['Andreas Säuberli', 'Simon Clematide']","Reading comprehension tests are used in a variety of applications, reaching from education to assessing the comprehensibility of simplified texts. However, creating such tests manually and ensuring their quality is difficult and time-consuming. In this paper, we explore how large language models (LLMs) can be used to generate and evaluate multiple-choice reading comprehension items. To this end, we compiled a dataset of German reading comprehension items and developed a new protocol for human and automatic evaluation, including a metric we call text informativity, which is based on guessability and answerability. We then used this protocol and the dataset to evaluate the quality of items generated by Llama 2 and GPT-4. Our results suggest that both models are capable of generating items of acceptable quality in a zero-shot setting, but GPT-4 clearly outperforms Llama 2. We also show that LLMs can be used for automatic evaluation by eliciting item reponses from them. In this scenario, evaluation results with GPT-4 were the most similar to human annotators. Overall, zero-shot generation with LLMs is a promising approach for generating and evaluating reading comprehension test items, in particular for languages without large amounts of available data.","Thu, 11 Apr 2024 13:11:21 UTC","본 논문에서는 대형 언어 모델을 사용하여 독해 문항을 생성하고 평가하는 방법을 탐구하며, GPT-4가 Llama 2보다 우수한 품질의 문항을 생성할 수 있음을 보여주었으며, LLMs를 사용한 독해 시험 문항 생성과 평가는 많은 데이터가 없는 언어에 특히 유용한 접근법임을 제시하였다.","['Reading comprehension tests', 'large language models (LLMs)', 'multiple-choice questions', 'German', 'text informativity', 'GPT-4', 'automatic evaluation.']"
46,46,46,Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics,"['Masashi Osada', 'Gustavo A. Garcia Ricardez', 'Yosuke Suzuki', 'Tadahiro Taniguchi']","Large language models (LLMs) and vision-language models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether LLMs can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) LLMs such as GPT-3.5 and GPT-4 can estimate an object's reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that GPT-4 can estimate an object's reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while GPT-3.5 obtained a competitive 19.9% compared to ResNet's 17.8%. These results suggest that the distributional semantics in LLMs and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.","Thu, 11 Apr 2024 13:09:37 UTC",": Large language models, vision-language models, robotics, reflectance estimation, object names, distributional semantics, GPT-3.5, GPT-4, CLIP, ResNet.

요약: 이 논문은 GPT-3.5와 GPT-4가 텍스트만을 입력으로 사용하여 물체의 반사율을 추정할 수 있으며, 또한 CLIP가 이미지를 기반으로 한 반사율 추정에서 일반화 능력을 향상시킬 수 있다는 것을 입증하였습니다.","['Large language models', 'vision-language models', 'robotics', 'reflectance estimation', 'object names', 'distributional semantics', 'GPT-3.5', 'GPT-4', 'CLIP', 'ResNet.']"
47,47,47,ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs,"['Lei Sun', 'Zhengwei Tao', 'Youdi Li', 'Hiroshi Arakawa']","The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.","Thu, 11 Apr 2024 12:16:16 UTC",": Large Language Models, knowledge graphs, natural language processing tasks, Observation-Driven Agent, KG reasoning abilities, global observation, recursive observation mechanism, state-of-the-art performance.

한줄 요약: 대형 언어 모델과 지식 그래프를 통합한 Observation-Driven Agent (ODA)는 지식 그래프 추론 능력을 향상시키는 새로운 인공지능 에이전트 프레임워크로, 순환적 관찰, 행동 및 반성 패러다임을 통해 상태-of-the-art 성능을 보여줌.","['Large Language Models', 'knowledge graphs', 'natural language processing tasks', 'Observation-Driven Agent', 'KG reasoning abilities', 'global observation', 'recursive observation mechanism', 'state-of-the-art performance.']"
48,48,48,Audio Dialogues: Dialogues dataset for audio and music understanding,"['Arushi Goel', 'Zhifeng Kong', 'Rafael Valle', 'Bryan Catanzaro']","Existing datasets for audio understanding primarily focus on single-turn interactions (i.e. audio captioning, audio question answering) for describing audio in natural language, thus limiting understanding audio via interactive dialogue. To address this gap, we introduce Audio Dialogues: a multi-turn dialogue dataset containing 163.8k samples for general audio sounds and music. In addition to dialogues, Audio Dialogues also has question-answer pairs to understand and compare multiple input audios together. Audio Dialogues leverages a prompting-based approach and caption annotations from existing datasets to generate multi-turn dialogues using a Large Language Model (LLM). We evaluate existing audio-augmented large language models on our proposed dataset to demonstrate the complexity and applicability of Audio Dialogues. Our code for generating the dataset will be made publicly available. Detailed prompts and generated dialogues can be found on the demo website this https URL.","Thu, 11 Apr 2024 10:08:34 UTC",": audio understanding, dataset, multi-turn dialogue, question-answer pairs, large language model, evaluation.

한국어 요약: 기존의 오디오 이해를 위한 데이터셋은 대부분 자연어로 오디오를 설명하는 단일 대화에 초점을 맞추고 있어 상호작용 대화를 통해 오디오를 이해하는 것을 제한하고 있습니다. 이 간극을 해결하기 위해, 우리는 163.8k개의 샘플을 포함하는 다중 대화 데이터셋인 Audio Dialogues를 소개합니다.","['audio understanding', 'dataset', 'multi-turn dialogue', 'question-answer pairs', 'large language model', 'evaluation.']"
49,49,49,Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain,"['Iker García-Ferrero', 'Rodrigo Agerri', 'Aitziber Atutxa Salazar', 'Elena Cabrio', 'Iker de la Iglesia', 'Alberto Lavelli', 'Bernardo Magnini', 'Benjamin Molinet', 'Johana Ramirez-Romero', 'German Rigau', 'Jose Maria Villa-Gonzalez', 'Serena Villata', 'Andrea Zaninello']","Research on language technology for the development of medical applications is currently a hot topic in Natural Language Understanding and Generation. Thus, a number of large language models (LLMs) have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction. While these LLMs display competitive performance on automated medical texts benchmarks, they have been pre-trained and evaluated with a focus on a single language (English mostly). This is particularly true of text-to-text models, which typically require large amounts of domain-specific pre-training data, often not easily accessible for many languages. In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish. This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we present two new evaluation benchmarks for all four languages with the aim of facilitating multilingual research in this domain. A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian benchmarks, while being competitive with current state-of-the-art LLMs in English.","Thu, 11 Apr 2024 10:01:32 UTC",": language technology, medical applications, large language models, multilingual corpus, text-to-text model, evaluation benchmarks.

한국어 요약: 의료 응용 프로그램 개발을 위한 언어 기술 연구는 현재 자연어 이해 및 생성 분야에서 핫한 주제이며, 이 논문에서는 의료 분야를 위한 첫 번째 오픈 소스 텍스트-텍스트 다국어 모델인 Medical mT5을 소개하고, 영어, 프랑스어, 이탈리아어, 스페인어의 의료 분야를 위한 최대 다국어 말뭉치를 사용하여 이 모델의 성능을 평가하였습니다.","['language technology', 'medical applications', 'large language models', 'multilingual corpus', 'text-to-text model', 'evaluation benchmarks.']"
50,50,50,UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs,"['Chaoqun He', 'Renjie Luo', 'Shengding Hu', 'Yuanqian Zhao', 'Jie Zhou', 'Hanghao Wu', 'Jiajie Zhang', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun']","Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements. The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher's workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration. UltraEval is now available for researchers publicly~\footnote{Website is at \url{this https URL}}.","Thu, 11 Apr 2024 09:17:12 UTC","이 논문은 UltraEval이라는 사용자 친화적인 경량 평가 프레임워크를 소개하며, 다양한 모델, 작업, 프롬프트 및 메트릭을 통합된 평가 워크플로 내에서 자유롭게 결합할 수 있도록 지원하며, 연구자들에게 공개되어 있다.","['Evaluation', 'Large Language Models (LLMs)', 'framework', 'lightweight', 'user-friendly', 'modularity', 'efficiency', 'models', 'data', 'metrics', 'composability', 'tasks', 'prompts', 'HTTP service', 'inference acceleration.']"
51,51,51,Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?,"['Marcel Hallgarten', 'Julian Zapata', 'Martin Stoll', 'Katrin Renz', 'Andreas Zell']","Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner's capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios.
A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization. We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark.","Thu, 11 Apr 2024 08:57:48 UTC","실세계 자율 주행 시스템은 희귀하고 다양한 교통 시나리오에서 안전한 결정을 내려야 하는데, 기존의 최첨단 플래너들은 주로 nuScenes나 nuPlan과 같은 실세계 데이터셋에서 평가되며, 이를 통해 기존 플래너들이 새로운 상황에 대한 일반화 능력을 평가하기 어렵다는 문제를 제기하고 있으며, 이에 대한 해결책으로 여러 가지 엣지 케이스와 도전적인 주행 시나리오를 포함한 새로운 폐","['autonomous driving systems', 'planners', 'real-world datasets', 'nuScenes', 'nuPlan', 'closed-loop benchmark', 'interPlan', 'edge cases', 'challenging driving scenarios', 'foundation models', 'large language models (LLM)', 'hybrid planner.']"
52,52,52,Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective,"['Yijie Chen', 'Yijin Liu', 'Fandong Meng', 'Yufeng Chen', 'Jinan Xu', 'Jie Zhou']","Code generation aims to understand the problem description and generate corresponding code snippets, where existing works generally decompose such complex tasks into intermediate steps by prompting strategies, such as Chain-of-Thought and its variants. While these studies have achieved some success, their effectiveness is highly dependent on the capabilities of advanced Large Language Models (LLMs) such as GPT-4, particularly in terms of API calls, which significantly limits their practical applicability. Consequently, how to enhance the code generation capabilities of small and medium-scale code LLMs without significantly increasing training costs is an appealing challenge. In this paper, we suggest that code comments are the natural logic pivot between natural language and code language and propose using comments to boost the code generation ability of code LLMs. Concretely, we propose MANGO (comMents As Natural loGic pivOts), including a comment contrastive training strategy and a corresponding logical comment decoding strategy. Experiments are performed on HumanEval and MBPP, utilizing StarCoder and WizardCoder as backbone models, and encompassing model parameter sizes between 3B and 7B. The results indicate that MANGO significantly improves the code pass rate based on the strong baselines. Meanwhile, the robustness of the logical comment decoding strategy is notably higher than the Chain-of-thoughts prompting. The code is publicly available at \url{this https URL}.","Thu, 11 Apr 2024 08:30:46 UTC","본 논문에서는 코드 주석을 활용하여 코드 생성 능력을 향상시키는 MANGO (comMents As Natural loGic pivOts)를 제안하였으며, 실험 결과는 MANGO가 강력한 기준선을 기반으로 코드 통과율을 크게 향상시킨 것을 보여주었습니다.","['Code generation', 'Large Language Models (LLMs)', 'GPT-4', 'API calls', 'code comments', 'MANGO', 'comment contrastive training strategy', 'logical comment decoding strategy.']"
53,53,53,"Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning","['Quanyu Long', 'Yin Wu', 'Wenya Wang', 'Sinno Jialin Pan']","In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format which helps LLMs to respond in desired label words. We then demonstrate this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL and find that retrieving the most semantically similar examples notably boosts model's discriminative capability.","Thu, 11 Apr 2024 08:20:10 UTC",": In-context Learning (ICL), large language models (LLMs), few-shot demonstrative examples, label space, format, discrimination, empirical decomposition, general-purpose LLMs, end-task performance, detailed instructions, retrieval mechanism, semantically similar examples.

Summary in Korean: 최근 분석 연구에서는 명확한 결과가 없었던데, 이 논문에서는 ICL의 성능을 라벨 공간, 형식, 구분 세 가지 차원으로 나누어 평가하고, 일반적인 LLMs에 대한 ICL의 효능을 다양한 작업들을 통해 확인하였습니다.","['In-context Learning (ICL)', 'large language models (LLMs)', 'few-shot demonstrative examples', 'label space', 'format', 'discrimination', 'empirical decomposition', 'general-purpose LLMs', 'end-task performance', 'detailed instructions', 'retrieval mechanism', 'semantically similar examples.']"
54,54,54,From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples,"['Robert Vacareanu', 'Vlad-Andrei Negru', 'Vasile Suciu', 'Mihai Surdeanu']","We analyze how well pre-trained large language models (e.g., Llama2, GPT-4, Claude 3, etc) can do linear and non-linear regression when given in-context examples, without any additional training or gradient updates. Our findings reveal that several large language models (e.g., GPT-4, Claude 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional supervised methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, Claude 3 outperforms many supervised methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of large language models scales with the number of in-context exemplars. We borrow from the notion of regret from online learning and empirically show that LLMs are capable of obtaining a sub-linear regret.","Thu, 11 Apr 2024 08:12:43 UTC","본 연구에서는 사전 훈련된 대형 언어 모델들이 인-컨텍스트 예시를 통해 선형 및 비선형 회귀 작업을 수행할 수 있는 능력을 조사하고, 그 결과로 GPT-4, Claude 3 등의 대형 언어 모델들이 전통적인 지도학습 방법들을 능가하는 성능을 보여주며, Claude 3은 Friedman #2 회귀 데이터셋에서 여러 지도학습 방법들을 능가하는 것을 발견하였고, 또한 인-컨텍스트 예시의 수에 따라 대형 언어 모델의 성능이 어떻게 변화하는지 조사하였","['pre-trained large language models', 'linear regression', 'non-linear regression', 'in-context examples', 'performance', 'supervised methods', 'regression dataset', 'regret', 'online learning.']"
55,55,55,Can Large Language Models Assess Serendipity in Recommender Systems?,"['Yu Tokutake', 'Kazushi Okamoto']","Serendipity-oriented recommender systems aim to counteract over-specialization in user preferences. However, evaluating a user's serendipitous response towards a recommended item can be challenging because of its emotional nature. In this study, we address this issue by leveraging the rich knowledge of large language models (LLMs), which can perform a variety of tasks. First, this study explored the alignment between serendipitous evaluations made by LLMs and those made by humans. In this investigation, a binary classification task was given to the LLMs to predict whether a user would find the recommended item serendipitously. The predictive performances of three LLMs on a benchmark dataset in which humans assigned the ground truth of serendipitous items were measured. The experimental findings reveal that LLM-based assessment methods did not have a very high agreement rate with human assessments. However, they performed as well as or better than the baseline methods. Further validation results indicate that the number of user rating histories provided to LLM prompts should be carefully chosen to avoid both insufficient and excessive inputs and that the output of LLMs that show high classification performance is difficult to interpret.","Thu, 11 Apr 2024 06:22:56 UTC",": Serendipity-oriented recommender systems, large language models (LLMs), predictive performance, binary classification, benchmark dataset, user rating histories.

한국어 요약: 본 연구는 대형 언어 모델(Large Language Models, LLMs)의 풍부한 지식을 활용하여 사용자의 우연한 반응을 예측하는 이슈를 다루었으며, LLMs의 예측 성능은 인간의 평가와 높은 일치율을 보이지 않았지만, 기존 방법보다 나은 성과를 보여주었으며, 사용자 평가 히스토리의 양은 신중히 선택되어야 하며, 높은 분류 성능을 보이는 LLMs의 결과는 해석하기 어렵다는 결과를 보여주었습니다.","['Serendipity-oriented recommender systems', 'large language models (LLMs)', 'predictive performance', 'binary classification', 'benchmark dataset', 'user rating histories.']"
56,56,56,Interactive Prompt Debugging with Sequence Salience,"['Ian Tenney', 'Ryan Mullins', 'Bin Du', 'Shree Pandya', 'Minsuk Kahng', 'Lucas Dixon']","We present Sequence Salience, a visual tool for interactive prompt debugging with input salience methods. Sequence Salience builds on widely used salience methods for text classification and single-token prediction, and extends this to a system tailored for debugging complex LLM prompts. Our system is well-suited for long texts, and expands on previous work by 1) providing controllable aggregation of token-level salience to the word, sentence, or paragraph level, making salience over long inputs tractable; and 2) supporting rapid iteration where practitioners can act on salience results, refine prompts, and run salience on the new output. We include case studies showing how Sequence Salience can help practitioners work with several complex prompting strategies, including few-shot, chain-of-thought, and constitutional principles. Sequence Salience is built on the Learning Interpretability Tool, an open-source platform for ML model visualizations, and code, notebooks, and tutorials are available at http://goo.gle/sequence-salience.","Thu, 11 Apr 2024 06:22:56 UTC","본 연구는 입력 중요도 방법을 활용한 상호 작용형 프롬프트 디버깅을 위한 시각적 도구인 Sequence Salience를 제시하며, 이 시스템은 긴 텍스트에 적합하며 복잡한 LLM 프롬프트를 디버깅할 수 있도록 설계되었으며, 신속한 반복 및 복잡한 프롬프팅 전략과 관련된 사례 연구를 포함하고 있습니다.","['Sequence Salience', 'interactive prompt debugging', 'input salience methods', 'LLM prompts', 'long texts', 'aggregation', 'rapid iteration', 'case studies', 'complex prompting strategies', 'Learning Interpretability Tool', 'ML model visualizations.']"
57,57,57,Neural Fault Injection: Generating Software Faults from Natural Language,"['Domenico Cotroneo', 'Pietro Liguori']","Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise. This paper introduces a novel methodology that harnesses the capabilities of Large Language Models (LLMs) augmented with Reinforcement Learning from Human Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the LLM's fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks. This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems.","Thu, 11 Apr 2024 05:59:16 UTC","본 논문은 전통적인 소프트웨어 결함 삽입 방법의 한계를 극복하기 위해 Large Language Models (LLMs)와 Reinforcement Learning from Human Feedback (RLHF)를 결합한 혁신적인 방법론을 제시하며, 이를 통해 소프트웨어 시스템의 신뢰성을 향상시키는 새로운 가능성을 열어줍니다.","['software fault injection', 'traditional methods', 'limitations', 'Large Language Models (LLMs)', 'Reinforcement Learning from Human Feedback (RLHF)', 'iterative refinement process', 'testers', 'fault scenarios', 'operational risks', 'manual effort', 'dependability of software systems']"
58,58,58,Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios,"['Yuan Zhang', 'Xiaomei Tao', 'Hanxu Ai', 'Tao Chen', 'Yanling Gan']","In the Massive Open Online Courses (MOOC) learning scenario, the semantic information of instructional videos has a crucial impact on learners' emotional state. Learners mainly acquire knowledge by watching instructional videos, and the semantic information in the videos directly affects learners' emotional states. However, few studies have paid attention to the potential influence of the semantic information of instructional videos on learners' emotional states. To deeply explore the impact of video semantic information on learners' emotions, this paper innovatively proposes a multimodal emotion recognition method by fusing video semantic information and physiological signals. We generate video descriptions through a pre-trained large language model (LLM) to obtain high-level semantic information about instructional videos. Using the cross-attention mechanism for modal interaction, the semantic information is fused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain the features containing the critical information of the three modes. The accurate recognition of learners' emotional states is realized through the emotion classifier. The experimental results show that our method has significantly improved emotion recognition performance, providing a new perspective and efficient method for emotion recognition research in MOOC learning scenarios. The method proposed in this paper not only contributes to a deeper understanding of the impact of instructional videos on learners' emotional states but also provides a beneficial reference for future research on emotion recognition in MOOC learning scenarios.","Thu, 11 Apr 2024 05:44:27 UTC",": Massive Open Online Courses (MOOC), semantic information, emotional states, video descriptions, multimodal emotion recognition, physiological signals, eye movement, PhotoPlethysmoGraphy (PPG), emotion classifier, experimental results.

한줄 요약: 이 논문은 MOOC 학습 시나리오에서 비디오 의미 정보가 학습자의 감정 상태에 중요한 영향을 미치는 것을 탐구하고, 비디오 의미 정보와 생리 신호를 융합한 다중 모달 감정 인식 방법을 제안하여 감정 인식 성능을 획기적으로 향상시켰다.","['Massive Open Online Courses (MOOC)', 'semantic information', 'emotional states', 'video descriptions', 'multimodal emotion recognition', 'physiological signals', 'eye movement', 'PhotoPlethysmoGraphy (PPG)', 'emotion classifier', 'experimental results.']"
59,59,59,"""Confidently Nonsensical?'': A Critical Survey on the Perspectives and Challenges of 'Hallucinations' in NLP","['Pranav Narayanan Venkit', 'Tatiana Chakravorti', 'Vipul Gupta', 'Heidi Biggs', 'Mukund Srinath', 'Koustava Goswami', 'Sarah Rajtmajer', 'Shomir Wilson']","We investigate how hallucination in large language models (LLM) is characterized in peer-reviewed literature using a critical examination of 103 publications across NLP research. Through a comprehensive review of sociological and technological literature, we identify a lack of agreement with the term `hallucination.' Additionally, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis underscores the necessity for explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.","Thu, 11 Apr 2024 03:51:29 UTC",": hallucination, large language models, NLP research, sociological literature, technological literature, survey, practitioners, AI, definitions, frameworks, challenges, society.

한국어 요약: NLP 연구를 통해 대규모 언어 모델에서의 환각을 조사한 결과, 환각에 대한 명확한 정의와 프레임워크의 필요성을 강조하며, 사회적 영향과 결과를 이해할 수 있는 주제적 이해를 제공합니다.","['hallucination', 'large language models', 'NLP research', 'sociological literature', 'technological literature', 'survey', 'practitioners', 'AI', 'definitions', 'frameworks', 'challenges', 'society.']"
60,60,60,WESE: Weak Exploration to Strong Exploitation for LLM Agents,"['Xu Huang', 'Weiwen Liu', 'Xiaolong Chen', 'Xingmei Wang', 'Defu Lian', 'Yasheng Wang', 'Ruiming Tang', 'Enhong Chen']","Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.","Thu, 11 Apr 2024 03:31:54 UTC","본 논문은 Weak Exploration to Strong Exploitation (WESE) 방법을 제안하여, LLM 에이전트의 성공률과 효율성을 향상시키는데 있어서 지식 그래프 기반 전략을 활용하는 방법을 제시하며, 이를 통해 네 가지 상호작용 벤치마크에서 성공률과 효율성을 크게 향상시킨다.","['large language models', 'intelligent agent', 'exploration', 'exploitation', 'open-world interactive tasks', 'Weak Exploration to Strong Exploitation (WESE)', 'knowledge graph-based strategy', 'success rate', 'efficiency']"
61,61,61,RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data,"['Yupeng Cao', 'Zhi Chen', 'Qingyun Pei', 'Fabrizio Dimino', 'Lorenzo Ausiello', 'Prashant Kumar', 'K.P. Subbalakshmi', 'Papa Momar Ndiaye']","The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering (Q\&A), and stock movement prediction (binary classification), with a notable gap in the application of LLMs for financial risk prediction. Addressing this gap, in this paper, we introduce \textbf{RiskLabs}, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data surrounding ECC release dates. Our approach involves a multi-stage process: initially extracting and analyzing ECC data using LLMs, followed by gathering and processing time-series data before the ECC dates to model and understand risk over different timeframes. Using multimodal fusion techniques, RiskLabs amalgamates these varied data features for comprehensive multi-task financial risk prediction. Empirical experiment results demonstrate RiskLab's effectiveness in forecasting both volatility and variance in financial markets. Through comparative experiments, we demonstrate how different data sources contribute to financial risk assessment and discuss the critical role of LLMs in this context. Our findings not only contribute to the AI in finance application but also open new avenues for applying LLMs in financial risk assessment.","Thu, 11 Apr 2024 03:14:50 UTC","금융 분야에서 LLMs를 활용한 RiskLabs 프레임워크가 금융 위험을 분석하고 예측하는 데 효과적임을 입증하며, 다양한 데이터 소스를 결합하여 다중 과제 금융 위험 예측을 제시하고 있습니다.","['Artificial Intelligence', 'large language models', 'finance', 'RiskLabs', 'financial risk prediction', 'Earnings Conference Calls', 'market-related time series data', 'multimodal fusion techniques', 'volatility', 'variance.']"
62,62,62,Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs,"['Kanchana Ranasinghe', 'Satya Narayan Shukla', 'Omid Poursaeed', 'Michael S. Ryoo', 'Tsung-Yu Lin']","Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.","Thu, 11 Apr 2024 03:09:34 UTC",": Large Language Models, visual-LLMs, spatial reasoning, localization awareness, coordinate-based instruction, fine-tuning objectives, pseudo-data generation, VQA, image and video domains.

한줄 요약: 시각-언어 모델에 공간 인식을 주입하기 위한 이미지 공간 좌표 기반의 fine-tuning 목표와 pseudo-data 생성 전략을 탐구하여, 시각-언어 모델의 성능을 향상시키고 시각-언어 작업에서 공간 인식 능력을 향상시키는 프레임워크를 제안하였습니다.","['Large Language Models', 'visual-LLMs', 'spatial reasoning', 'localization awareness', 'coordinate-based instruction', 'fine-tuning objectives', 'pseudo-data generation', 'VQA', 'image and video domains.']"
63,63,63,CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models,"['Sheng Wang', 'Tianming Du', 'Katherine Fischer', 'Gregory E Tasian', 'Justin Ziemba', 'Joanie M Garratt', 'Hersh Sagreiya', 'Yong Fan']","Computer-aided diagnosis systems hold great promise to aid radiologists and clinicians in radiological clinical practice and enhance diagnostic accuracy and efficiency. However, the conventional systems primarily focus on delivering diagnostic results through text report generation or medical image classification, positioning them as standalone decision-makers rather than helpers and ignoring radiologists' expertise. This study introduces an innovative paradigm to create an assistive co-pilot system for empowering radiologists by leveraging Large Language Models (LLMs) and medical image analysis tools. Specifically, we develop a collaborative framework to integrate LLMs and quantitative medical image analysis results generated by foundation models with radiologists in the loop, achieving efficient and safe generation of radiology reports and effective utilization of computational power of AI and the expertise of medical professionals. This approach empowers radiologists to generate more precise and detailed diagnostic reports, enhancing patient outcomes while reducing the burnout of clinicians. Our methodology underscores the potential of AI as a supportive tool in medical diagnostics, promoting a harmonious integration of technology and human expertise to advance the field of radiology.","Thu, 11 Apr 2024 01:33:45 UTC","본 연구는 Large Language Models (LLMs)와 의료 이미지 분석 도구를 활용하여 방사선과 전문가들의 협업을 통해 더 정확하고 자세한 진단 보고서를 생성하는 방법을 소개하며, AI의 계산 능력과 의료 전문가의 전문 지식을 효과적으로 활용하여 환자 결과를 향상시키고 의료진의 탈진을 줄이는 방법을 제시합니다.","['Computer-aided diagnosis systems', 'radiologists', 'Large Language Models (LLMs)', 'medical image analysis', 'collaborative framework', 'radiology reports', 'AI', 'medical professionals.']"
64,64,64,JetMoE: Reaching Llama2 Performance with 0.1M Dollars,"['Yikang Shen', 'Zhen Guo', 'Tianle Cai', 'Zengyi Qin']","Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs. The model weights are publicly available at this https URL.","Thu, 11 Apr 2024 00:52:39 UTC",": Large Language Models (LLMs), JetMoE-8B, cost-effective training, Sparsely-gated Mixture-of-Experts (SMoE) architecture, open foundation models.

한줄 요약: 젯모이-8B는 저렴한 가격에 훈련된 새로운 대형 언어 모델로, 놀라운 성능을 보여주며, 투명성을 통해 협력과 발전을 촉구하는 데 도움이 되고 있습니다.","['Large Language Models (LLMs)', 'JetMoE-8B', 'cost-effective training', 'Sparsely-gated Mixture-of-Experts (SMoE) architecture', 'open foundation models.']"
65,65,65,ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past,"['Van Pham', 'Scott Cunningham']","This study investigates whether OpenAI's ChatGPT-3.5 and ChatGPT-4 can accurately forecast future events using two distinct prompting strategies. To evaluate the accuracy of the predictions, we take advantage of the fact that the training data at the time of experiment stopped at September 2021, and ask about events that happened in 2022 using ChatGPT-3.5 and ChatGPT-4. We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters that share events that have happened to them, but after ChatGPT's training data had been collected. Concentrating on events in 2022, we prompted ChatGPT to engage in storytelling, particularly within economic contexts. After analyzing 100 prompts, we discovered that future narrative prompts significantly enhanced ChatGPT-4's forecasting accuracy. This was especially evident in its predictions of major Academy Award winners as well as economic trends, the latter inferred from scenarios where the model impersonated public figures like the Federal Reserve Chair, Jerome Powell. These findings indicate that narrative prompts leverage the models' capacity for hallucinatory narrative construction, facilitating more effective data synthesis and extrapolation than straightforward predictions. Our research reveals new aspects of LLMs' predictive capabilities and suggests potential future applications in analytical contexts.","Thu, 11 Apr 2024 00:03:03 UTC","이 연구는 ChatGPT-3.5 및 ChatGPT-4가 미래 이벤트를 정확하게 예측할 수 있는지를 두 가지 다른 프롬프팅 전략을 사용하여 조사하였으며, 미래 서술을 통해 모델의 예측 정확도를 향상시키는 것을 발견하였습니다.","['OpenAI', 'ChatGPT-3.5', 'ChatGPT-4', 'forecasting', 'prompting strategies', 'training data', 'future events', 'narratives', 'economic contexts', 'Academy Award winners', 'economic trends', 'predictive capabilities', 'LLMs.']"
66,66,66,BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks,"['Ruijia Cheng', 'Titus Barik', 'Alan Leung', 'Fred Hohman', 'Jeffrey Nichols']","Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs). However, they encounter difficulties in understanding and working with code produced by LLMs. To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user prompts and code generation. We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code. Through 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas. We discuss the implications of our findings for UI-centric interactive paradigm in code generation LLMs.","Wed, 10 Apr 2024 23:28:09 UTC",": machine learning tutorials, computational notebooks, code generation technologies, large language models (LLMs), UI-based scaffolds, BISCUIT, JupyterLab, user studies, novices, semantic representation, prompt engineering, interactive paradigm.

Summary in Korean: 컴퓨터 노트북에서의 기계 학습 튜토리얼에서 대규모 언어 모델을 기반으로 한 코드 생성 기술을 사용하는 초보자들이 LLM이 생성한 코드를 이해하고 작업하는 데 어려움을 겪는 문제를 해결하기 위해, BISCUIT을 소개하고, LLM에 의해 생성된 코드에 대한 사용자 의도와 이해를 돕는 UI 기반의 지원을 제공하는 연구입니다.","['machine learning tutorials', 'computational notebooks', 'code generation technologies', 'large language models (LLMs)', 'UI-based scaffolds', 'BISCUIT', 'JupyterLab', 'user studies', 'novices', 'semantic representation', 'prompt engineering', 'interactive paradigm.']"
67,67,67,Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving,"['Chenyang An', 'Zhibo Chen', 'Qihao Ye', 'Emily First', 'Letian Peng', 'Jiayun Zhang', 'Zihan Wang', 'Sorin Lerner', 'Jingbo Shang']","Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.","Wed, 10 Apr 2024 23:01:45 UTC","최근 자동 증명에 대한 최신 연구에서는 (대규모) 언어 모델을 활용하여 전술(즉, 증명 단계)을 생성하고 증명 상태를 탐색하는 효과를 보여주었으며, 실패한 탐색 경로에서 학습하는 모델의 이점을 입증하였습니다.","['Automated Theorem Proving', 'language model', 'tactics', 'proof states', 'inference', 'training', 'failed attempts', 'trial-and-error data', 'intuitionistic propositional logic', 'Lean', 'correctness of proofs', 'TrialMaster.']"
68,68,68,LLMs in Biomedicine: A study on clinical Named Entity Recognition,"['Masoud Monajatipoor', 'Jiaxin Yang', 'Joel Stremmel', 'Melika Emami', 'Fazlolah Mohaghegh', 'Mozhdeh Rouhsedaghat', 'Kai-Wei Chang']","Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the medical domain by exploring strategies to enhance their performance for the Named-Entity Recognition (NER) task. Specifically, our study reveals the importance of meticulously designed prompts in biomedicine. Strategic selection of in-context examples yields a notable improvement, showcasing ~15-20\% increase in F1 score across all benchmark datasets for few-shot clinical NER. Additionally, our findings suggest that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER. Leveraging a medical knowledge base, our proposed method inspired by Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for zero-shot clinical NER. We will release the code upon publication.","Wed, 10 Apr 2024 22:26:26 UTC",": Large Language Models, NLP tasks, biomedicine, Named-Entity Recognition (NER), prompts, in-context examples, F1 score, few-shot clinical NER, external resources, medical knowledge base, Retrieval-Augmented Generation (RAG), zero-shot clinical NER.

한줄 요약: 의료 분야에서 LLMs의 성능을 향상시키기 위해 설계된 프롬프트의 중요성을 밝히고, 외부 자원을 활용한 전략적 선택으로 의료 NER 작업의 성능을 향상시키는 연구 결과를 발표한 논문입니다.","['Large Language Models', 'NLP tasks', 'biomedicine', 'Named-Entity Recognition (NER)', 'prompts', 'in-context examples', 'F1 score', 'few-shot clinical NER', 'external resources', 'medical knowledge base', 'Retrieval-Augmented Generation (RAG)', 'zero-shot clinical NER.']"
69,69,69,"""We Need Structured Output"": Towards User-centered Constraints on Large Language Model Output","['Michael Xieyang Liu', 'Frederick Liu', 'Alexander J. Fiannaca', 'Terry Koo', 'Lucas Dixon', 'Michael Terry', 'Carrie J. Cai']","Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.","Wed, 10 Apr 2024 21:37:07 UTC",": Large language models, creative responses, output constraints, user-centered perspective, use cases, structured format, semantic guidelines, stylistic guidelines, user experience, constraint prototyping tool.

한줄 요약: 대형 언어 모델은 창의적이고 다양한 응답을 생성할 수 있지만, 특정 형식이나 표준을 따르도록 출력을 제한하는 것이 중요하며 이를 통해 개발자의 작업을 효율적으로 할 수 있고 사용자 경험을 향상시킬 수 있다.","['Large language models', 'creative responses', 'output constraints', 'user-centered perspective', 'use cases', 'structured format', 'semantic guidelines', 'stylistic guidelines', 'user experience', 'constraint prototyping tool.']"
70,70,70,Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs,"['Bibek Upadhayay', 'Vahid Behzadan']","Large Language Models (LLMs) are increasingly being developed and applied, but their widespread use faces challenges. These include aligning LLMs' responses with human values to prevent harmful outputs, which is addressed through safety training methods. Even so, bad actors and malicious users have succeeded in attempts to manipulate the LLMs to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights. Another challenge is the multilingual capabilities of LLMs, which enable the model to understand and respond in multiple languages. Consequently, attackers exploit the unbalanced pre-training datasets of LLMs in different languages and the comparatively lower model performance in low-resource languages than high-resource ones. As a result, attackers use a low-resource languages to intentionally manipulate the model to create harmful responses. Many of the similar attack vectors have been patched by model providers, making the LLMs more robust against language-based manipulation. In this paper, we introduce a new black-box attack vector called the \emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses. Our experiments with five different models, namely Google's Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse.","Tue, 9 Apr 2024 18:29:42 UTC ",": Large Language Models, safety training methods, harmful outputs, multilingual capabilities, low-resource languages, black-box attack, Sandwich attack, model providers, public good.

요약: 대형 언어 모델(Large Language Models)은 안전 훈련 방법을 통해 인간 가치와 일치시키는 것에 대한 도전과 다국어 능력, 낮은 자원 언어, 샌드위치 공격과 같은 공격에 대응해야 합니다.","['Large Language Models', 'safety training methods', 'harmful outputs', 'multilingual capabilities', 'low-resource languages', 'black-box attack', 'Sandwich attack', 'model providers', 'public good.']"
71,71,71,Explaining EDA synthesis errors with LLMs,"['Siyu Qiu', 'Benjamin Tan', 'Hammond Pearce']","Training new engineers in digital design is a challenge, particularly when it comes to teaching the complex electronic design automation (EDA) tooling used in this domain. Learners will typically deploy designs in the Verilog and VHDL hardware description languages to Field Programmable Gate Arrays (FPGAs) from Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains (Quartus Prime and Vivado, respectively). These tools are complex and difficult to use -- yet, as they are the tools used in industry, they are an essential first step in this space. In this work, we examine how recent advances in artificial intelligence may be leveraged to address aspects of this challenge. Specifically, we investigate if Large Language Models (LLMs), which have demonstrated text comprehension and question-answering capabilities, can be used to generate novice-friendly explanations of compile-time synthesis error messages from Quartus Prime and Vivado. To perform this study we generate 936 error message explanations using three OpenAI LLMs over 21 different buggy code samples. These are then graded for relevance and correctness, and we find that in approximately 71% of cases the LLMs give correct & complete explanations suitable for novice learners.","Sun, 7 Apr 2024 07:12:16 UTC ",": digital design, engineering training, electronic design automation (EDA), Verilog, VHDL, Field Programmable Gate Arrays (FPGAs), Quartus Prime, Vivado, artificial intelligence, Large Language Models (LLMs), error messages.

한국어 요약: 새로운 엔지니어들을 디지털 디자인에 훈련하는 것은 어려운 과제이며, 이 연구에서는 최근 인공지능의 발전을 활용하여 Quartus Prime와 Vivado로부터 컴파일 시간 합성 오류 메시지에 대한 초보자 친화적 설명을 생성할 수 있는지 조사하였고, 결과적으로 LLMs가 약 71%의 경우에 정확하고 완전한 설명을 제공할 수 있음을 발견하였습니다.","['digital design', 'engineering training', 'electronic design automation (EDA)', 'Verilog', 'VHDL', 'Field Programmable Gate Arrays (FPGAs)', 'Quartus Prime', 'Vivado', 'artificial intelligence', 'Large Language Models (LLMs)', 'error messages.']"
72,72,72,Goal-guided Generative Prompt Injection Attack on Large Language Models,"['Chong Zhang', 'Mingyu Jin', 'Qinkai Yu', 'Chengzhi Liu', 'Haochen Xue', 'Xiaobo Jin']","Current large language models (LLMs) provide a strong foundation for large-scale user-oriented natural language tasks. A large number of users can easily inject adversarial text or instructions through the user interface, thus causing LLMs model security challenges. Although there is currently a large amount of research on prompt injection attacks, most of these black-box attacks use heuristic strategies. It is unclear how these heuristic strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we redefine the goal of the attack: to maximize the KL divergence between the conditional probabilities of the clean text and the adversarial text. Furthermore, we prove that maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between the embedded representation x and x' of the clean text and the adversarial text when the conditional probability is a Gaussian distribution and gives a quantitative relationship on x and x'. Then we designed a simple and effective goal-guided generative prompt injection strategy (G2PIA) to find an injection text that satisfies specific constraints to achieve the optimal attack effect approximately. It is particularly noteworthy that our attack method is a query-free black-box attack method with low computational cost. Experimental results on seven LLM models and four datasets show the effectiveness of our attack method.","Sat, 6 Apr 2024 06:17:10 UTC ","대규모 언어 모델에 대한 공격 방법 중 하나인 프롬프트 주입 공격의 효율적인 방법을 제시하고, KL 발산 및 Mahalanobis 거리를 최대화하여 모델의 강건성을 향상시키는 연구 결과를 소개합니다.","['large language models', 'adversarial text', 'prompt injection attacks', 'model security challenges', 'KL divergence', 'Mahalanobis distance', 'generative prompt injection strategy', 'black-box attack method.']"
73,73,73,Improving Retrieval for RAG based Question Answering Models on Financial Documents,"['Spurthi Setty', 'Katherine Jijo', 'Eden Chung', 'Natan Vidra']","The effectiveness of Large Language Models (LLMs) in generating accurate responses relies heavily on the quality of input provided, particularly when employing Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by sourcing the most relevant text chunk(s) to base queries upon. Despite the significant advancements in LLMs' response quality in recent years, users may still encounter inaccuracies or irrelevant answers; these issues often stem from suboptimal text chunk retrieval by RAG rather than the inherent capabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine the RAG process. This paper explores the existing constraints of RAG pipelines and introduces methodologies for enhancing text retrieval. It delves into strategies such as sophisticated chunking techniques, query expansion, the incorporation of metadata annotations, the application of re-ranking algorithms, and the fine-tuning of embedding algorithms. Implementing these approaches can substantially improve the retrieval quality, thereby elevating the overall performance and reliability of LLMs in processing and responding to queries.","Sat, 23 Mar 2024 00:49:40 UTC",": Large Language Models, Retrieval Augmented Generation, text chunk retrieval, methodologies, query expansion, metadata annotations, re-ranking algorithms, embedding algorithms.

Summary in Korean: 대형 언어 모델(Large Language Models, LLMs)의 효과적인 응답 생성 능력은 검색 증강 생성(Retrieval Augmented Generation, RAG) 기술을 적용할 때 특히 입력 품질에 의존하며, 이 논문은 RAG 파이프라인의 제약 사항을 탐구하고 텍스트 검색 향상을 위한 방법론을 소개함으로써 LLMs의 전체 성능과 신뢰성을 향상시키는 방법을 제시한다.","['Large Language Models', 'Retrieval Augmented Generation', 'text chunk retrieval', 'methodologies', 'query expansion', 'metadata annotations', 're-ranking algorithms', 'embedding algorithms.']"
74,74,74,Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers,"['Kunal Sawarkar', 'Abhilasha Mangal', 'Shivam Raj Solanki']","Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q\&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the 'Blended RAG' method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q\&A datasets like SQUAD, even surpassing fine-tuning performance.","Fri, 22 Mar 2024 17:13:46 UTC",": Retrieval-Augmented Generation (RAG), Large Language Models (LLM), Generative Q&A, Retrievers, Blended RAG, semantic search techniques, Dense Vector indexes, Sparse Encoder indexes, hybrid query strategies, IR datasets, NQ, TREC-COVID datasets, Blended Retriever, SQUAD, fine-tuning performance.

Summary in Korean: 본 논문에서는 'Blended RAG' 방법을 제안하여 Dense Vector 인덱스와 Sparse Encoder 인덱스와 하이브리드 쿼리 전략을 활용하여 정보 검색 결과를 개선하고, NQ 및 TREC-COVID 데이터셋과 같은 IR 데이터셋에 새로운 기준을 제시하며, 이를 RAG 시스템에 확장하여 SQUAD와 같은 Generative Q&A 데이터셋에서 우수한 결과를 달성하고 세밀한 조정 성능을 능가함을 입증합니다.","['Retrieval-Augmented Generation (RAG)', 'Large Language Models (LLM)', 'Generative Q&A', 'Retrievers', 'Blended RAG', 'semantic search techniques', 'Dense Vector indexes', 'Sparse Encoder indexes', 'hybrid query strategies', 'IR datasets', 'NQ', 'TREC-COVID datasets', 'Blended Retriever', 'SQUAD', 'fine-tuning performance.']"
75,75,75,Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions,"['Akash Ghosh', 'Arkadeep Acharya', 'Sriparna Saha', 'Vinija Jain', 'Aman CHadha']","The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.","Tue, 20 Feb 2024 18:57:34 UTC",": Large Language Models (LLMs), AI revolution, Vision-Language Models (VLMs), image captioning, visual question answering, classification, multimodal inputs, benchmark datasets.

Summary in Korean: 대형 언어 모델의 등장으로 AI 혁명의 방향이 크게 바뀌었지만, 이러한 모델들은 주로 텍스트 정보를 처리하는 데 능숙한 한계를 보이며, 이를 극복하기 위해 시각 능력을 통합한 VLM이 등장하였으며, 본 조사 논문에서는 VLM의 주요 발전을 탐구하고, 이를 세 가지 범주로 분류하여 각 모델의 기본 아키텍처, 훈련 데이터 소스, 강점 및 한계를 철저히 분석하고, 미래 연구 방향과 가능성을 강조하며 다","['Large Language Models (LLMs)', 'AI revolution', 'Vision-Language Models (VLMs)', 'image captioning', 'visual question answering', 'classification', 'multimodal inputs', 'benchmark datasets.']"
76,76,76,Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention,"['Tsendsuren Munkhdalai', 'Manaal Faruqui', 'Siddharth Gopal']","This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.","Wed, 10 Apr 2024 16:18:42 UTC",": Transformer-based Large Language Models, Infini-attention, compressive memory, masked local attention, long-term linear attention, long-context language modeling benchmarks, passkey context block retrieval, book summarization tasks, bounded memory parameters, streaming inference.

Summary in Korean: 이 연구는 Transformer 기반의 대규모 언어 모델을 무한히 긴 입력에 대해 제한된 메모리와 계산으로 확장하는 효율적인 방법을 소개하며, Infini-attention이라는 새로운 어텐션 기술을 도입하여 효과적인 접근 방식을 제시하고 있다.","['Transformer-based Large Language Models', 'Infini-attention', 'compressive memory', 'masked local attention', 'long-term linear attention', 'long-context language modeling benchmarks', 'passkey context block retrieval', 'book summarization tasks', 'bounded memory parameters', 'streaming inference.']"
77,77,77,Continuous Language Model Interpolation for Dynamic and Controllable Text Generation,"['Sara Kangaslahti', 'David Alvarez-Melis']","As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse -- and often changing -- user preferences. For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously.","Wed, 10 Apr 2024 15:55:07 UTC",": Large language models, adaptation methods, linear weight interpolation, user preferences, fine-tuning, model outputs.

요약: 대형 언어 모델이 인기를 얻으면서, 사용자 기반 애플리케이션을 위해 모델이 다양하고 변화하는 사용자 선호도에 동적으로 적응해야 하는 어려운 경우에 대해 선형 가중치 보간을 기반으로 한 적응 방법을 활용하여 모델 출력을 다양한 스타일적 특성에 대해 예측 가능하고 세밀하게 제어할 수 있음을 실험적으로 보여줌.","['Large language models', 'adaptation methods', 'linear weight interpolation', 'user preferences', 'fine-tuning', 'model outputs.']"
78,78,78,From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications,"['Yongqiang Ma', 'Lizhi Qing', 'Jiawei Liu', 'Yangyang Kang', 'Yue Zhang', 'Wei Lu', 'Xiaozhong Liu', 'Qikai Cheng']","Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, ``Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.","Wed, 10 Apr 2024 15:46:08 UTC",": large language models, evaluation, human-centered, AI-powered writing assistance, Revision Distance, revision edits, self-explained text evaluation.

한국어 요약: 대형 언어 모델 평가는 실용적인 응용 프로그램에 중요한데, 기존의 방법은 사용자 경험을 무시하는 수치적 점수를 제공하는 반면, 본 연구는 인간 중심의 평가로 초점을 옮겨 AI 기반 글쓰기 지원 애플리케이션에서 제안된 'Revision Distance' 메트릭은 인간의 쓰기 과정을 모방한 수정 편집을 제안하여 자체 설명 가능한 텍스트 평가 결과를 제공하며, 다른 메트릭보다 더 세부적인 피드백을 제공하고 텍스트 간 차이를 명확히 구분할 수 있음을","['large language models', 'evaluation', 'human-centered', 'AI-powered writing assistance', 'Revision Distance', 'revision edits', 'self-explained text evaluation.']"
79,79,79,Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs,"['Bowen Jin', 'Chulin Xie', 'Jiawei Zhang', 'Kashob Kumar Roy', 'Yu Zhang', 'Suhang Wang', 'Yu Meng', 'Jiawei Han']","Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently. The code is available at this https URL.","Wed, 10 Apr 2024 15:41:53 UTC","대규모 언어 모델(Large language models)을 그래프와 함께 사용하여 지식을 이용해 성능을 향상시키는 연구에서, Graph-CoT 프레임워크가 기존 방법들보다 우수한 성과를 보여주며 연구를 진행하고 있습니다.","['Large language models (LLMs)', 'hallucinations', 'knowledge-intensive tasks', 'external knowledge corpora', 'text-attributed graph', 'Graph Reasoning Benchmark dataset', 'Graph Chain-of-thought (Graph-CoT)', 'LLM-graph interaction', 'systematic experiments.']"
80,80,80,Dynamic Generation of Personalities with Large Language Models,"['Jianzhi Liu', 'Hexiang Gu', 'Tianyu Zheng', 'Liuyu Xiang', 'Huijia Wu', 'Jie Fu', 'Zhaofeng He']","In the realm of mimicking human deliberation, large language models (LLMs) show promising performance, thereby amplifying the importance of this research area. Deliberation is influenced by both logic and personality. However, previous studies predominantly focused on the logic of LLMs, neglecting the exploration of personality aspects. In this work, we introduce Dynamic Personality Generation (DPG), a dynamic personality generation method based on Hypernetworks. Initially, we embed the Big Five personality theory into GPT-4 to form a personality assessment machine, enabling it to evaluate characters' personality traits from dialogues automatically. We propose a new metric to assess personality generation capability based on this evaluation method. Then, we use this personality assessment machine to evaluate dialogues in script data, resulting in a personality-dialogue dataset. Finally, we fine-tune DPG on the personality-dialogue dataset. Experiments prove that DPG's personality generation capability is stronger after fine-tuning on this dataset than traditional fine-tuning methods, surpassing prompt-based GPT-4.","Wed, 10 Apr 2024 15:17:17 UTC",": large language models, deliberation, logic, personality, Dynamic Personality Generation, Hypernetworks, Big Five personality theory, GPT-4, fine-tuning.

한국어 요약: 이 연구에서는 Hypernetworks를 기반으로 한 Dynamic Personality Generation(DPG) 방법을 소개하며, GPT-4에 대다섯 요인 성격 이론을 포함하여 인물의 성격 특성을 자동으로 평가할 수 있는 성격 평가 기계를 구축하고, 이를 통해 대화 스크립트 데이터에서 성격-대화 데이터셋을 구축하고 DPG를 이 데이터셋에서 세밀하게 조정함으로써 GPT-4를 능가하는 더 강력한 성격 생성 능력을 입증합니다.","['large language models', 'deliberation', 'logic', 'personality', 'Dynamic Personality Generation', 'Hypernetworks', 'Big Five personality theory', 'GPT-4', 'fine-tuning.']"
81,81,81,Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study,['Alessandro Stolfo'],"We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs). In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model's pre-training data. Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers. Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.","Wed, 10 Apr 2024 14:50:10 UTC",": groundedness, long-form question answering (LFQA), retrieval-augmented large language models (LLMs), datasets, model families, ungrounded sentences, model size, decoding strategy, instruction tuning, hallucinations, robust mechanisms.

한국어 요약: 이 연구는 장문형 질문 응답(LFQA)에서 검색 증강 대형 언어 모델(LLMs)에 대한 실증 연구를 제시합니다.","['groundedness', 'long-form question answering (LFQA)', 'retrieval-augmented large language models (LLMs)', 'datasets', 'model families', 'ungrounded sentences', 'model size', 'decoding strategy', 'instruction tuning', 'hallucinations', 'robust mechanisms.']"
82,82,82,A Mathematical Theory for Learning Semantic Languages by Abstract Learners,"['Kuo-Yu Liao', 'Cheng-Shang Chang', 'Y.-W. Peter Hong']","Recent advances in Large Language Models (LLMs) have demonstrated the emergence of capabilities (learned skills) when the number of system parameters and the size of training data surpass certain thresholds. The exact mechanisms behind such phenomena are not fully understood and remain a topic of active research. Inspired by the skill-text bipartite graph model presented in [1] for modeling semantic language, we develop a mathematical theory to explain the emergence of learned skills, taking the learning (or training) process into account. Our approach models the learning process for skills in the skill-text bipartite graph as an iterative decoding process in Low-Density Parity Check (LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density evolution analysis, we demonstrate the emergence of learned skills when the ratio of the size of training texts to the number of skills exceeds a certain threshold. Our analysis also yields a scaling law for testing errors relative to the size of training texts. Upon completion of the training, we propose a method for semantic compression and discuss its application in semantic communication.","Wed, 10 Apr 2024 13:50:46 UTC",": Large Language Models, learned skills, training data, mathematical theory, bipartite graph model, iterative decoding process, Low-Density Parity Check codes, Irregular Repetition Slotted ALOHA, density evolution analysis, scaling law, semantic compression, semantic communication.

한국어 요약: 최근의 대형 언어 모델(Large Language Models)의 발전은 일정 임계값을 초과하는 시스템 매개변수 수와 교육 데이터 크기에서 능력(습득된 기술)의 발생을 입증하였으며, 이러한 현상의 정확한 메커니즘은 완전히 이해되지 않았으며 활발하게 연구되는 주제입니다.","['Large Language Models', 'learned skills', 'training data', 'mathematical theory', 'bipartite graph model', 'iterative decoding process', 'Low-Density Parity Check codes', 'Irregular Repetition Slotted ALOHA', 'density evolution analysis', 'scaling law', 'semantic compression', 'semantic communication.']"
83,83,83,WordDecipher: Enhancing Digital Workspace Communication with Explainable AI for Non-native English Speakers,"['Yuexi Chen', 'Zhicheng Liu']","Non-native English speakers (NNES) face challenges in digital workspace communication (e.g., emails, Slack messages), often inadvertently translating expressions from their native languages, which can lead to awkward or incorrect usage. Current AI-assisted writing tools are equipped with fluency enhancement and rewriting suggestions; however, NNES may struggle to grasp the subtleties among various expressions, making it challenging to choose the one that accurately reflects their intent. Such challenges are exacerbated in high-stake text-based communications, where the absence of non-verbal cues heightens the risk of misinterpretation. By leveraging the latest advancements in large language models (LLM) and word embeddings, we propose WordDecipher, an explainable AI-assisted writing tool to enhance digital workspace communication for NNES. WordDecipher not only identifies the perceived social intentions detected in users' writing, but also generates rewriting suggestions aligned with users' intended messages, either numerically or by inferring from users' writing in their native language. Then, WordDecipher provides an overview of nuances to help NNES make selections. Through a usage scenario, we demonstrate how WordDecipher can significantly enhance an NNES's ability to communicate her request, showcasing its potential to transform workspace communication for NNES.","Wed, 10 Apr 2024 13:40:29 UTC",": Non-native English speakers, digital workspace communication, AI-assisted writing tools, large language models, WordDecipher.

한국어 요약: WordDecipher는 최신 언어 모델과 단어 임베딩을 활용하여 비영어 원어민 사용자들의 디지털 작업 공간 커뮤니케이션을 향상시키는 설명 가능한 AI 보조 작성 도구로, 사용자의 의도에 부합하는 재작성 제안을 제공하고 언어의 세부 차이를 설명하여 선택을 도와줍니다.","['Non-native English speakers', 'digital workspace communication', 'AI-assisted writing tools', 'large language models', 'WordDecipher.']"
84,84,84,Event Grounded Criminal Court View Generation withCooperative (Large) Language Models,"['Linan Yue', 'Qi Liu', 'Lili Zhao', 'Li Wang', 'Weibo Gao', 'Yanqing An']","With the development of legal intelligence, Criminal Court View Generation has attracted much attention as a crucial task of legal intelligence, which aims to generate concise and coherent texts that summarize case facts and provide explanations for verdicts. Existing researches explore the key information in case facts to yield the court views. Most of them employ a coarse-grained approach that partitions the facts into broad segments (e.g., verdict-related sentences) to make predictions. However, this approach fails to capture the complex details present in the case facts, such as various criminal elements and legal events. To this end, in this paper, we propose an Event Grounded Generation (EGG) method for criminal court view generation with cooperative (Large) Language Models, which introduces the fine-grained event information into the generation. Specifically, we first design a LLMs-based extraction method that can extract events in case facts without massive annotated events. Then, we incorporate the extracted events into court view generation by merging case facts and events. Besides, considering the computational burden posed by the use of LLMs in the extraction phase of EGG, we propose a LLMs-free EGG method that can eliminate the requirement for event extraction using LLMs in the inference phase. Extensive experimental results on a real-world dataset clearly validate the effectiveness of our proposed method.","Wed, 10 Apr 2024 13:31:07 UTC",": Legal intelligence, Criminal Court View Generation, Event Grounded Generation (EGG), Cooperative (Large) Language Models, Fine-grained event information, Case facts, Verdicts, Criminal elements, Legal events, Experimental results, Real-world dataset

Summary in Korean: 법적 지능의 발전으로 인해, 형사 법원 의견 생성은 사건 사실을 요약하고 판결에 대한 설명을 제공하는 간결하고 일관된 텍스트를 생성하는 중요한 작업으로 주목받고 있습니다. 이 논문에서는 형사 법원 의견 생성을 위한 이벤트 중심 생성(EGG) 방법을 소개하며, 이를 통해 사건 사실과 이벤트를 융합하여 법원 의견을 생성하는 방법을 제안합니다.","['Legal intelligence', 'Criminal Court View Generation', 'Event Grounded Generation (EGG)', 'Cooperative (Large) Language Models', 'Fine-grained event information', 'Case facts', 'Verdicts', 'Criminal elements', 'Legal events', 'Experimental results', 'Real-world dataset']"
85,85,85,XNLIeu: a dataset for cross-lingual NLI in Basque,"['Maite Heredia', 'Julen Etxaniz', 'Muitze Zulaika', 'Xabier Saralegi', 'Jeremy Barnes', 'Aitor Soroa']","XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.","Wed, 10 Apr 2024 13:19:56 UTC","본 논문에서는 저자들이 영어 XNLI 코퍼스를 바스크어로 기계 번역하고 전문적인 후편집을 거쳐 새로운 데이터셋인 XNLIeu를 개발하였으며, 이를 통해 바스크어 자연어 이해 능력에 대한 교차언어 전이 학습 방법을 평가하는 실험을 진행한 결과를 보고하고 있습니다.","['XNLI', 'Natural Language Inference', 'Basque', 'transfer-learning', 'cross-lingual', 'NLU', 'dataset', 'machine translation', 'post-edition', 'LLMs', 'professional', 'strategy', 'translation', 'code', 'datasets.']"
86,86,86,Quati: A Brazilian Portuguese Information Retrieval Dataset from Native Speakers,"['Mirelle Bueno', 'Eduardo Seiti de Oliveira', 'Rodrigo Nogueira', 'Roberto A. Lotufo', 'Jayr Alencar Pereira']","Despite Portuguese being one of the most spoken languages in the world, there is a lack of high-quality information retrieval datasets in that language. We present Quati, a dataset specifically designed for the Brazilian Portuguese language. It comprises a collection of queries formulated by native speakers and a curated set of documents sourced from a selection of high-quality Brazilian Portuguese websites. These websites are frequented more likely by real users compared to those randomly scraped, ensuring a more representative and relevant corpus. To label the query-document pairs, we use a state-of-the-art LLM, which shows inter-annotator agreement levels comparable to human performance in our assessments. We provide a detailed description of our annotation methodology to enable others to create similar datasets for other languages, providing a cost-effective way of creating high-quality IR datasets with an arbitrary number of labeled documents per query. Finally, we evaluate a diverse range of open-source and commercial retrievers to serve as baseline systems. Quati is publicly available at this https URL and all scripts at this https URL .","Wed, 10 Apr 2024 12:42:28 UTC",": Portuguese language, information retrieval datasets, Quati dataset, Brazilian Portuguese, native speakers, high-quality websites, annotation methodology, LLM, inter-annotator agreement, baseline systems.

한국어 요약: 이 연구에서는 세계에서 가장 많이 사용되는 포르투갈어에 대한 고품질 정보 검색 데이터셋의 부족함을 지적하고, 브라질 포르투갈어를 위해 특별히 설계된 Quati 데이터셋을 소개합니다.","['Portuguese language', 'information retrieval datasets', 'Quati dataset', 'Brazilian Portuguese', 'native speakers', 'high-quality websites', 'annotation methodology', 'LLM', 'inter-annotator agreement', 'baseline systems.']"
87,87,87,Advancing Real-time Pandemic Forecasting Using Large Language Models: A COVID-19 Case Study,"['Hongru Du', 'Jianan Zhao', 'Yang Zhao', 'Shaochong Xu', 'Xihong Lin', 'Yiran Chen', 'Lauren M. Gardner', 'Hao Frank Yang']","Forecasting the short-term spread of an ongoing disease outbreak is a formidable challenge due to the complexity of contributing factors, some of which can be characterized through interlinked, multi-modality variables such as epidemiological time series data, viral biology, population demographics, and the intersection of public policy and human behavior. Existing forecasting model frameworks struggle with the multifaceted nature of relevant data and robust results translation, which hinders their performances and the provision of actionable insights for public health decision-makers. Our work introduces PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs) that reformulates real-time forecasting of disease spread as a text reasoning problem, with the ability to incorporate real-time, complex, non-numerical information that previously unattainable in traditional forecasting models. This approach, through a unique AI-human cooperative prompt design and time series representation learning, encodes multi-modal data for LLMs. The model is applied to the COVID-19 pandemic, and trained to utilize textual public health policies, genomic surveillance, spatial, and epidemiological time series data, and is subsequently tested across all 50 states of the U.S. Empirically, PandemicLLM is shown to be a high-performing pandemic forecasting framework that effectively captures the impact of emerging variants and can provide timely and accurate predictions. The proposed PandemicLLM opens avenues for incorporating various pandemic-related data in heterogeneous formats and exhibits performance benefits over existing models. This study illuminates the potential of adapting LLMs and representation learning to enhance pandemic forecasting, illustrating how AI innovations can strengthen pandemic responses and crisis management in the future.","Wed, 10 Apr 2024 12:22:03 UTC",": disease outbreak, forecasting model, Large Language Models, pandemic, COVID-19, public health policies, genomic surveillance, spatial data, epidemiological time series data, variants.

한글 요약: 본 연구는 다중 언어 모델을 활용한 새로운 팬데믹 예측 프레임워크인 PandemicLLM을 소개하며, COVID-19 팬데믹을 대상으로 텍스트 추론 문제로 실시간 예측을 재구성하여 신속하고 정확한 예측을 제공하는 고성능 팬데믹 예측 프레임워크를 제시하고 있습니다.","['disease outbreak', 'forecasting model', 'Large Language Models', 'pandemic', 'COVID-19', 'public health policies', 'genomic surveillance', 'spatial data', 'epidemiological time series data', 'variants.']"
88,88,88,Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy,"['Yijin Liu', 'Fandong Meng', 'Jie Zhou']","Recently, dynamic computation methods have shown notable acceleration for Large Language Models (LLMs) by skipping several layers of computations through elaborate heuristics or additional predictors. However, in the decoding process of existing approaches, different samples are assigned different computational budgets, which cannot guarantee a stable and precise acceleration effect. Furthermore, existing approaches generally skip multiple contiguous layers at the bottom or top of the layers, leading to a drastic change in the model's layer-wise representations, and thus a consequent performance degeneration. Therefore, we propose a Unified Layer Skipping strategy, which selects the number of layers to skip computation based solely on the target speedup ratio, and then skips the corresponding number of intermediate layer computations in a balanced manner. Since the Unified Layer Skipping strategy is independent of input samples, it naturally supports popular acceleration techniques such as batch decoding and KV caching, thus demonstrating more practicality for real-world applications. Experimental results on two common tasks, i.e., machine translation and text summarization, indicate that given a target speedup ratio, the Unified Layer Skipping strategy significantly enhances both the inference performance and the actual model throughput over existing dynamic approaches.","Wed, 10 Apr 2024 12:12:07 UTC",": Large Language Models (LLMs), dynamic computation methods, acceleration, decoding process, computational budgets, Unified Layer Skipping strategy, layer-wise representations, performance degeneration, speedup ratio, batch decoding, KV caching, machine translation, text summarization.

한국어 요약: 최근에는 대형 언어 모델(Large Language Models, LLMs)을 가속화하는 동적 계산 방법이 여러 계층의 계산을 건너뛰어 상세한 휴리스틱이나 추가 예측기를 통해 주목할 만한 가속화를 보여주고 있지만, 기존 방법의 디코딩 과정에서는 서로 다른 샘플에게 다른 계산 예산이 할당되어 안정적이고 정확한 가속 효과를 보장할 수 없습니다. 그러므로 저희는 대상 가속 비율에 따라 계산을 건너뛰는 계","['Large Language Models (LLMs)', 'dynamic computation methods', 'acceleration', 'decoding process', 'computational budgets', 'Unified Layer Skipping strategy', 'layer-wise representations', 'performance degeneration', 'speedup ratio', 'batch decoding', 'KV caching', 'machine translation', 'text summarization.']"
89,89,89,MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models,"['Rahul Mehta', 'Andrew Hoblitzell', ""Jack O'Keefe"", 'Hyeju Jang', 'Vasudeva Varma']","Hallucinations in large language models (LLMs) have recently become a significant problem. A recent effort in this direction is a shared task at Semeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. This paper describes our winning solution ranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks respectively. We propose a meta-regressor framework of LLMs for model evaluation and integration that achieves the highest scores on the leaderboard. We also experiment with various transformer-based models and black box methods like ChatGPT, Vectara, and others. In addition, we perform an error analysis comparing GPT4 against our best model which shows the limitations of the former.","Wed, 10 Apr 2024 11:56:01 UTC","최근 대형 언어 모델에서 발생한 환각 문제에 대한 연구로, Semeval 2024 Task 6인 SHROOM을 통해 모델에 대한 메타-회귀 프레임워크를 제안하고, ChatGPT, Vectara 등의 모델과 GPT4와의 한계를 비교한 논문을 소개합니다.","['Hallucinations', 'large language models', 'Semeval 2024 Task 6', 'SHROOM', 'meta-regressor framework', 'transformer-based models', 'ChatGPT', 'Vectara', 'GPT4', 'model evaluation.']"
90,90,90,GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications,"['Shishir G. Patil', 'Tianjun Zhang', 'Vivian Fang', 'Noppapon C.', 'Roy Huang', 'Aaron Hao', 'Martin Casado', 'Joseph E. Gonzalez', 'Raluca Ada Popa', 'Ion Stoica']","Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. We argue that in many cases, ""post-facto validation"" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned ""pre-facto validation"" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. We release GoEX at this https URL.","Wed, 10 Apr 2024 11:17:33 UTC",": Large Language Models (LLMs), post-facto validation, code comprehension, autonomous collaboration, undo feature, damage confinement, Gorilla Execution Engine (GoEX).

한문 요약: 이 연구는 대형 언어 모델이 실제 응용 프로그램 및 서비스에서 도구와 상호 작용하고 실행하는 역할을 넘어 대화 시스템 내에서 정보를 제공하는 전통적인 역할을 발전시키고 있다고 주장하며, 이를 위해 ""사후 검증"" 방식으로 LLM-generated actions의 올바름과 적절성을 보장하기 위한 방법을 연구하고 있음을 설명합니다.","['Large Language Models (LLMs)', 'post-facto validation', 'code comprehension', 'autonomous collaboration', 'undo feature', 'damage confinement', 'Gorilla Execution Engine (GoEX).']"
91,91,91,Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation,"['Thomas Merth', 'Qichen Fu', 'Mohammad Rastegari', 'Mahyar Najibi']","Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the ""distraction phenomenon,"" where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, superposition prompting, which can be directly applied to pre-trained transformer-based LLMs without the need for fine-tuning. At a high level, superposition prompting allows the LLM to process input documents in parallel prompt paths, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates an 93x reduction in compute time while improving accuracy by 43\% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.","Wed, 10 Apr 2024 11:03:17 UTC",이 논문은 대규모 언어 모델의 한계를 극복하기 위해 제안된 새로운 RAG 프롬프팅 방법인 superposition prompting이 병렬 프롬프트 경로를 통해 입력 문서를 처리하여 시간 효율성을 향상시키고 정확도를 향상시키는 데 효과적임을 입증하였습니다.,"['large language models', 'drawbacks', 'inference cost', 'distraction phenomenon', 'superposition prompting', 'transformer-based LLMs', 'time efficiency', 'question-answering benchmarks', 'accuracy improvement.']"
92,92,92,Vision-Language Model-based Physical Reasoning for Robot Liquid Perception,"['Wenqiang Lai', 'Yuan Gao', 'Tin Lun Lam']","There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% -- achieved by the best-performing vision-only variant -- to 86.0%.","Wed, 10 Apr 2024 10:49:43 UTC",": large language models, robotic tasks, physical world, vision-language models, GPT-4V, liquid objects, multimodal perception, viscosity, physical attributes, interactions.

한문 요약: 최근의 연구에서는 대규모 언어 모델을 활용하여 로봇 작업에 적용하는 것에 대한 관심이 높아졌으며, 물체를 인식하는 데 이미지를 활용하는 새로운 패러다임이 제안되었습니다.","['large language models', 'robotic tasks', 'physical world', 'vision-language models', 'GPT-4V', 'liquid objects', 'multimodal perception', 'viscosity', 'physical attributes', 'interactions.']"
93,93,93,Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing,"['Mohamadreza Rostami', 'Marco Chilese', 'Shaza Zeitouni', 'Rahul Kande', 'Jeyavijayan Rajendran', 'Ahmad-Reza Sadeghi']","Modern computing systems heavily rely on hardware as the root of trust. However, their increasing complexity has given rise to security-critical vulnerabilities that cross-layer at-tacks can exploit. Traditional hardware vulnerability detection methods, such as random regression and formal verification, have limitations. Random regression, while scalable, is slow in exploring hardware, and formal verification techniques are often concerned with manual effort and state explosions. Hardware fuzzing has emerged as an effective approach to exploring and detecting security vulnerabilities in large-scale designs like modern processors. They outperform traditional methods regarding coverage, scalability, and efficiency. However, state-of-the-art fuzzers struggle to achieve comprehensive coverage of intricate hardware designs within a practical timeframe, often falling short of a 70% coverage threshold. We propose a novel ML-based hardware fuzzer, ChatFuzz, to address this challenge. Ourapproach leverages LLMs like ChatGPT to understand processor language, focusing on machine codes and generating assembly code sequences. RL is integrated to guide the input generation process by rewarding the inputs using code coverage metrics. We use the open-source RISCV-based RocketCore processor as our testbed. ChatFuzz achieves condition coverage rate of 75% in just 52 minutes compared to a state-of-the-art fuzzer, which requires a lengthy 30-hour window to reach a similar condition coverage. Furthermore, our fuzzer can attain 80% coverage when provided with a limited pool of 10 simulation instances/licenses within a 130-hour window. During this time, it conducted a total of 199K test cases, of which 6K produced discrepancies with the processor's golden model. Our analysis identified more than 10 unique mismatches, including two new bugs in the RocketCore and discrepancies from the RISC-V ISA Simulator.","Wed, 10 Apr 2024 09:28:54 UTC",": hardware, security vulnerabilities, hardware fuzzing, machine learning, ChatFuzz, RISCV-based RocketCore processor, coverage rate, test cases, bugs.

한줄 요약: 본 연구는 ChatFuzz라는 기계 학습 기반 하드웨어 퍼저를 제안하여, RISCV 기반 RocketCore 프로세서에서 보안 취약점을 발견하고 테스트 케이스를 분석하여 새로운 버그를 발견했습니다.","['hardware', 'security vulnerabilities', 'hardware fuzzing', 'machine learning', 'ChatFuzz', 'RISCV-based RocketCore processor', 'coverage rate', 'test cases', 'bugs.']"
94,94,94,Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora?,"['Miriam Anschütz', 'Edoardo Mosca', 'Georg Groh']","Text simplification seeks to improve readability while retaining the original content and meaning. Our study investigates whether pre-trained classifiers also maintain such coherence by comparing their predictions on both original and simplified inputs. We conduct experiments using 11 pre-trained models, including BERT and OpenAI's GPT 3.5, across six datasets spanning three languages. Additionally, we conduct a detailed analysis of the correlation between prediction change rates and simplification types/strengths. Our findings reveal alarming inconsistencies across all languages and models. If not promptly addressed, simplified inputs can be easily exploited to craft zero-iteration model-agnostic adversarial attacks with success rates of up to 50%","Wed, 10 Apr 2024 09:02:33 UTC",": Text simplification, readability, coherence, pre-trained classifiers, BERT, GPT 3.5, datasets, languages, prediction change rates, adversarial attacks.

Summary in Korean: 텍스트 단순화는 가독성을 향상시키면서도 원본 내용과 의미를 유지하려고 합니다. 본 연구는 11개의 사전 훈련된 모델을 사용하여 원본과 단순화된 입력에 대한 예측을 비교함으로써 이러한 일관성을 유지하는지 조사합니다. 결과는 모든 언어와 모델에서 알람을 일으키는 불일치를 보여주며, 즉시 조치를 취하지 않으면 단순화된 입력이 쉽게 악용되어 성공률이 최대 50%인 영제 모델에 적대적 공격이 가능하다는 것을 보여줍","['Text simplification', 'readability', 'coherence', 'pre-trained classifiers', 'BERT', 'GPT 3.5', 'datasets', 'languages', 'prediction change rates', 'adversarial attacks.']"
95,95,95,Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge,"['Li Zhou', 'Taelin Karidi', 'Nicolas Garneau', 'Yong Cao', 'Wanlong Liu', 'Wenyu Chen', 'Daniel Hershcovich']","Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains.","Wed, 10 Apr 2024 08:49:27 UTC","이 연구는 LLMs의 문화적 편향을 분석하고, 음식 분야에 초점을 맞춘 FmLAMA를 소개하며, 다양한 언어로 문화적 지식을 탐색하여 LLMs의 성능을 향상시키는 방법을 연구하였습니다.","['cultural biases', 'Large Language Models (LLMs)', 'Food domain', 'FmLAMA', 'multilingual dataset', 'cultural knowledge', 'United States', 'cultural context', 'model performance', 'diverse datasets.']"
96,96,96,Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation,"['Ruotong Pan', 'Boxi Cao', 'Hongyu Lin', 'Xianpei Han', 'Jia Zheng', 'Sirui Wang', 'Xunliang Cai', 'Le Sun']","The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models' capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance. Moreover, our model supports customized credibility, offering a wide range of potential applications.","Wed, 10 Apr 2024 07:56:26 UTC",": large language models, Retrieval-Augmented Generation (RAG), knowledge bottlenecks, hallucinations, flawed information, Credibility-aware Generation (CAG), data transformation framework, credibility, benchmark, robust performance, noisy documents.

한줄 요약: 본 논문에서는 Credibility-aware Generation (CAG)이라는 신뢰도를 갖는 데이터 변환 프레임워크를 제안하며, 이를 통해 모델이 신뢰도를 이해하고 활용하여 안정적인 성능을 유지할 수 있음을 실험 결과로 입증하였습니다.","['large language models', 'Retrieval-Augmented Generation (RAG)', 'knowledge bottlenecks', 'hallucinations', 'flawed information', 'Credibility-aware Generation (CAG)', 'data transformation framework', 'credibility', 'benchmark', 'robust performance', 'noisy documents.']"
97,97,97,Adapting LLaMA Decoder to Vision Transformer,"['Jiahao Wang', 'Wenqi Shao', 'Mengzhao Chen', 'Chengyue Wu', 'Yong Liu', 'Kaipeng Zhang', 'Songyang Zhang', 'Kai Chen', 'Ping Luo']","This work examines whether decoder-only Transformers such as LLaMA, which were originally designed for large language models (LLMs), can be adapted to the computer vision field. We first ""LLaMAfy"" a standard ViT step-by-step to align with LLaMA's architecture, and find that directly applying a casual mask to the self-attention brings an attention collapse issue, resulting in the failure to the network training. We suggest to reposition the class token behind the image tokens with a post-sequence class token technique to overcome this challenge, enabling causal self-attention to efficiently capture the entire image's information. Additionally, we develop a soft mask strategy that gradually introduces a casual mask to the self-attention at the onset of training to facilitate the optimization behavior. The tailored model, dubbed as image LLaMA (iLLaMA), is akin to LLaMA in architecture and enables direct supervised learning. Its causal self-attention boosts computational efficiency and learns complex representation by elevating attention map ranks. iLLaMA rivals the performance with its encoder-only counterparts, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to ~310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%. Extensive experiments demonstrate iLLaMA's reliable properties: calibration, shape-texture bias, quantization compatibility, ADE20K segmentation and CIFAR transfer learning. We hope our study can kindle fresh views to visual model design in the wave of LLMs. Pre-trained models and codes are available here.","Wed, 10 Apr 2024 06:30:08 UTC","이 연구는 LLaMA와 같은 디코더 전용 트랜스포머가 컴퓨터 비전 분야에 적용될 수 있는지 조사하고, 새로운 이미지 LLaMA(iLLaMA) 모델을 개발하여 이미지 정보를 효과적으로 캡처하고 뛰어난 성능을 보여주는 것을 보여주고 있습니다.","['decoder-only Transformers', 'LLaMA', 'computer vision', 'ViT', 'attention collapse issue', 'class token', 'soft mask strategy', 'iLLaMA', 'supervised learning', 'computational efficiency', 'ImageNet', 'pre-training', 'segmentation', 'transfer learning.']"
98,98,98,Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems,"['Zhengyuan Liu', 'Stella Xin Yin', 'Geyu Lin', 'Nancy F. Chen']","Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.","Wed, 10 Apr 2024 06:03:13 UTC",": Intelligent Tutoring Systems, large language models, conversational ITSs, dialogic teaching, student engagement, persona, cognitive aspects, noncognitive aspects, personality-aware student simulation, language learning scenario, multi-aspect validation, experimental results.

한줄 요약: 이 연구는 대화형 학습 시스템을 통해 학생의 언어 능력과 성격 특성에 따라 다양한 학생 응답을 생성하고 교사의 적응형 지원 전략을 유도할 수 있다는 것을 실험 결과를 통해 입증하였습니다.","['Intelligent Tutoring Systems', 'large language models', 'conversational ITSs', 'dialogic teaching', 'student engagement', 'persona', 'cognitive aspects', 'noncognitive aspects', 'personality-aware student simulation', 'language learning scenario', 'multi-aspect validation', 'experimental results.']"
99,99,99,Leveraging open-source models for legal language modeling and analysis: a case study on the Indian constitution,"['Vikhyath Gupta (Vidya Jyothi Institute of Technology', 'Hyderabad', 'Telangana', 'India)', 'Srinivasa Rao P (Curlvee TechnoLabs', 'Hyderabad', 'Telangana', 'India)']","In recent years, the use of open-source models has gained immense popularity in various fields, including legal language modelling and analysis. These models have proven to be highly effective in tasks such as summarizing legal documents, extracting key information, and even predicting case outcomes. This has revolutionized the legal industry, enabling lawyers, researchers, and policymakers to quickly access and analyse vast amounts of legal text, saving time and resources. This paper presents a novel approach to legal language modeling (LLM) and analysis using open-source models from Hugging Face. We leverage Hugging Face embeddings via LangChain and Sentence Transformers to develop an LLM tailored for legal texts. We then demonstrate the application of this model by extracting insights from the official Constitution of India. Our methodology involves preprocessing the data, splitting it into chunks, using ChromaDB and LangChainVectorStores, and employing the Google/Flan-T5-XXL model for analysis. The trained model is tested on the Indian Constitution, which is available in PDF format. Our findings suggest that our approach holds promise for efficient legal language processing and analysis.","Wed, 10 Apr 2024 05:35:47 UTC","최근 법률 언어 모델링 및 분석에서 오픈 소스 모델의 사용이 급격히 증가하고 있으며, Hugging Face의 오픈 소스 모델을 활용한 새로운 접근 방식을 통해 인도 헌법을 분석하는 효율적인 법률 언어 처리 및 분석 방법이 제안되었다.","['open-source models', 'legal language modelling', 'analysis', 'summarizing legal documents', 'extracting key information', 'predicting case outcomes', 'legal industry', 'Hugging Face', 'LangChain', 'Sentence Transformers', 'LLM', 'Constitution of India', 'ChromaDB', 'LangChainVectorStores', 'Google/Flan-T5-XXL model.']"
